{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Romain Zimmer\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNet:\n",
    "    \n",
    "    def __init__(self, sess, N, J, embedding_size = 128, lr = 1e-3, training_mode = 'supervised', alpha = 0.03):\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        \n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        if training_mode == 'supervised':\n",
    "            self.build_supervised()\n",
    "            with tf.variable_scope('Supervised_optimizer'):\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "                \n",
    "        elif training_mode == 'reinforce':\n",
    "            self.alpha = 0.03\n",
    "            self.build_reinforce()\n",
    "            with tf.variable_scope('Reinforce_optimizer'):\n",
    "                self.train_op =  tf.train.RMSPropOptimizer(lr).minimize(self.reinforce_loss)\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError(\"Unknown training mode: %s\" % training_mode))\n",
    "        \n",
    "        print(\"All variables\")\n",
    "        for var in tf.global_variables():\n",
    "            print(var)\n",
    "            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            self.identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            self.embedded_identities = tf.nn.embedding_lookup(self.identity_embeddings, inputs)\n",
    "        \n",
    "            \n",
    "        return tf.unstack(self.embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None:\n",
    "            \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            else:\n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = h\n",
    "            \n",
    "            b2 = tf.get_variable('b2', shape = (1, self.embedding_size))\n",
    "            \n",
    "            dense1 =tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1) + b1)\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2) + b2)\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1, self.J))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "        \n",
    "            return policy_logit\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_actions(self, log_proba):\n",
    "        \n",
    "        action = tf.multinomial(log_proba, num_samples = 1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], None, None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        self.h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        self.layers = {'h0_seq': h0_seq, 'h1_seq': h1_seq, 'c1_seq':c1_seq, 'h2_seq': self.h2_seq}\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in self.h2_seq]\n",
    "        \n",
    "        self.log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.policy_logit_seq]\n",
    "        \n",
    "        self.action_seq = [self.sample_actions(log_proba) for log_proba in self.log_proba_seq]\n",
    "        \n",
    "        self.one_hot_action_seq = [tf.one_hot(action, depth = self.J) for action in self.action_seq]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_supervised(self):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_mean(supervised_loss_seq)\n",
    "        \n",
    "        \n",
    "    def supervised_train(self, X, y, val_X, val_y, env, batch_size = 32, epochs = 1):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            np.random.shuffle(data_inds)\n",
    "            supervised_loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm_notebook(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                _, supervised_loss, one_hot_action_seq = sess.run([self.train_op, self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "                supervised_loss_sum += supervised_loss * batch_size\n",
    "                reward_sum += env.get_reward(one_hot_action_seq)\n",
    "            \n",
    "            print(\"loss = %f\" % (supervised_loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            val_supervised_loss, val_one_hot_action_seq = sess.run([self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss))\n",
    "            print('val reward = %f' % (env.get_reward(val_one_hot_action_seq) / val_n))\n",
    "            \n",
    "    def build_baseline(self, h):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        with tf.variable_scope('Baseline', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  1))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1,))\n",
    "            \n",
    "            \n",
    "            baseline = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "            \n",
    "            return baseline\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        self.indices = tf.placeholder(tf.int32, shape = (None, 2))\n",
    "        \n",
    "        self.shape = tf.placeholder(tf.int32, shape =(2,))\n",
    "        \n",
    "        self.baselines = tf.concat([self.build_baseline(h2) for h2 in self.h2_seq], axis = 1)\n",
    "        scattered_baselines = tf.scatter_nd(self.indices, tf.reshape(self.baselines, [-1]), shape = self.shape)\n",
    "                    \n",
    "        self.reward_values = tf.placeholder(tf.float32, shape = (None,))\n",
    "        scattered_reward_values = tf.scatter_nd(self.indices, self.reward_values, shape = self.shape)\n",
    "        scattered_reward_values_cumsum = tf.cumsum(scattered_reward_values, axis = 0, reverse = True)\n",
    "        \n",
    "        self.baseline_values =  tf.placeholder(tf.float32, shape = (None, self.J))\n",
    "        scattered_baseline_values = tf.scatter_nd(self.indices, tf.reshape(self.baseline_values, [-1]), shape = self.shape)\n",
    "        \n",
    "        \n",
    "        self.action_taken = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_action_taken = tf.unstack(self.action_taken, axis = 1)\n",
    "        \n",
    "        self.neg_log_p = tf.transpose(tf.concat([tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_action_taken[j],\n",
    "                                                    logits=self.policy_logit_seq[j]) for j in range(self.J)], axis = 0))\n",
    "        \n",
    "        scattered_neg_log_p = tf.scatter_nd(self.indices, tf.reshape(self.neg_log_p, [-1]), shape = self.shape)\n",
    "        \n",
    "        #surrogate loss (- dtheta)\n",
    "        self.reinforce_loss = tf.reduce_sum(tf.multiply(scattered_neg_log_p, scattered_reward_values_cumsum - scattered_baseline_values))\n",
    "        self.reinforce_loss += self.alpha * tf.reduce_sum(tf.square(scattered_reward_values_cumsum - scattered_baselines))\n",
    "        \n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        action_seq, baselines= self.sess.run([self.action_seq, self.baselines], {self.inputs: [state]})\n",
    "        \n",
    "        return [a[0,0] for a in action_seq], baselines\n",
    "    \n",
    "    def reinforce_train(self, env, n_episodes, T):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        \n",
    "        history = {'reward' : [],  'loss': []}    \n",
    "        \n",
    "        for _ in tqdm_notebook(range(n_episodes), \"REINFORCE\"):\n",
    "            \n",
    "            \n",
    "            state_seq, action_seq, reward_seq, baseline_seq = policy_rollout(T, env, self)\n",
    "            episode_len = reward_seq.shape[0]\n",
    "            \n",
    "            history['reward'].append(np.mean(reward_seq))\n",
    "            \n",
    "            repeated_t = np.repeat(np.arange(episode_len), self.J)\n",
    "            \n",
    "            indices = np.vstack([repeated_t, state_seq.ravel()]) .T\n",
    "                \n",
    "            feed_dict = {}\n",
    "            feed_dict[self.inputs] = state_seq\n",
    "            feed_dict[self.indices] = indices\n",
    "            feed_dict[self.shape] = [episode_len, self.N]\n",
    "            feed_dict[self.reward_values] = np.repeat(reward_seq, self.J)\n",
    "            feed_dict[self.baseline_values] = baseline_seq\n",
    "            feed_dict[self.action_taken] = action_seq\n",
    "            \n",
    "            _, loss = self.sess.run([self.train_op, self.reinforce_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "            \n",
    "        return history\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeverEnv:\n",
    "    \n",
    "    def __init__(self, N, J):\n",
    "        \n",
    "        self.J = J\n",
    "        self.N = N\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return state, terminal_state\n",
    "    \n",
    "    def get_reward(self, one_hot_action_seq):        \n",
    "        \n",
    "        reward = np.sum(np.sum(one_hot_action_seq, axis = 0) > 0) /self.J\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        \n",
    "        next_state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        one_hot_action_seq = np.zeros((self.J, self.J))\n",
    "        one_hot_action_seq[range(self.J), action] = 1\n",
    "        reward = self.get_reward(one_hot_action_seq)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return next_state, reward, terminal_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation for supervised learning\n",
    "def generate_data(n, N, J):\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    y= np.empty((n,J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.random.choice(N, size = J, replace = False)\n",
    "        sorted_args = np.argsort(X[i])\n",
    "        y[i] = np.argsort(sorted_args)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode generation for reinforcement learning\n",
    "def policy_rollout(T, env, agent):\n",
    "    \n",
    "    state_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    baseline_seq = []\n",
    "    \n",
    "    \n",
    "    state, terminal_state = env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while not terminal_state and t < T:\n",
    "        t +=1\n",
    "        \n",
    "        state_seq.append(state)\n",
    "        action, baseline = agent.take_action(state)\n",
    "        \n",
    "        state, reward, terminal_state = env.step(state, action)\n",
    "        \n",
    "        \n",
    "        action_seq.append(action)\n",
    "        reward_seq.append(reward)\n",
    "        baseline_seq.append(baseline)\n",
    "        \n",
    "    return np.array(state_seq), np.array(action_seq), np.array(reward_seq), np.squeeze(np.array(baseline_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "J = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size * 1000\n",
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(500, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables\n",
      "<tf.Variable 'Encoder/identity_embeddings:0' shape=(500, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b2:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b2:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/W:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/b:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Baseline/W:0' shape=(128, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Baseline/b:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/RMSProp:0' shape=(500, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/RMSProp_1:0' shape=(500, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/RMSProp:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/RMSProp_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/RMSProp:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/RMSProp_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/RMSProp:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/RMSProp_1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/RMSProp:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/RMSProp_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/RMSProp:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/RMSProp_1:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/b/RMSProp:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/b/RMSProp_1:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/W/RMSProp:0' shape=(128, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/W/RMSProp_1:0' shape=(128, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/b/RMSProp:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/b/RMSProp_1:0' shape=(1,) dtype=float32_ref>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911cfe2abe4d4d20b2b9c10c0fa18092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='REINFORCE', style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNet(sess, N, J, lr = 1e-3, embedding_size= 128, training_mode = 'reinforce')\n",
    "    env = LeverEnv(N, J)\n",
    "    \n",
    "    #commNet.supervised_train(X, y, val_X, val_y, env, batch_size = batch_size, epochs = 1)\n",
    "    #rv = sess.run([commNet.embedded_identities, commNet.identity_embeddings, commNet.layers], feed_dict = {commNet.inputs: val_X[0:2], commNet.targets: val_y[0:2]})\n",
    "    \n",
    "    history = commNet.reinforce_train(env, n_episodes = 100, T =64)\n",
    "    #state_seq, action_seq, reward_seq, proba_seq = policy_rollout(T = 32, env = env, agent = commNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216dff6e208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216dddf6240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x216ddde0780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = 100\n",
    "plt.plot(np.convolve(history['reward'], np.ones(W), mode= 'valid')/W)\n",
    "plt.show()\n",
    "plt.plot(history['reward'], '*')\n",
    "plt.show()\n",
    "plt.plot(history['loss'], '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.22577909 0.26917762 0.21279874 0.16583364 0.12641096]\n",
      "[0.22016849 0.22563423 0.22212386 0.19095235 0.14112104]\n",
      "[0.21960501 0.23955436 0.22702406 0.1854479  0.12836868]\n",
      "[0.23304532 0.20993459 0.22559714 0.1791272  0.15229584]\n",
      "[0.20872119 0.25417063 0.2264204  0.18819685 0.12249094]\n",
      "\n",
      "[0.21246645 0.20228745 0.23133822 0.20866685 0.145241  ]\n",
      "[0.22078106 0.24104035 0.21198149 0.18721826 0.13897899]\n",
      "[0.22285442 0.24080707 0.21916117 0.18383345 0.13334392]\n",
      "[0.21217397 0.23280925 0.22768702 0.18952763 0.13780206]\n",
      "[0.19502863 0.266822   0.22554079 0.19426489 0.11834362]\n",
      "\n",
      "[0.21161614 0.22226155 0.22272208 0.20031306 0.14308715]\n",
      "[0.21610449 0.21933357 0.22805688 0.19413775 0.14236726]\n",
      "[0.21205838 0.24951085 0.22500771 0.18818292 0.12524009]\n",
      "[0.21903999 0.209828   0.22463156 0.1948897  0.15161072]\n",
      "[0.23773776 0.18132898 0.2302288  0.18018381 0.1705207 ]\n",
      "\n",
      "[0.22067125 0.21549195 0.22589959 0.18399791 0.15393928]\n",
      "[0.21357127 0.24210793 0.22301947 0.1944984  0.12680286]\n",
      "[0.19097054 0.26365468 0.22255631 0.20049433 0.12232405]\n",
      "[0.20663887 0.21646273 0.22846274 0.19785467 0.15058103]\n",
      "[0.22137447 0.24524052 0.22064109 0.18040796 0.13233596]\n",
      "\n",
      "[0.21273632 0.27325147 0.21687391 0.18208349 0.11505479]\n",
      "[0.21884504 0.17979847 0.22955675 0.19940954 0.17239021]\n",
      "[0.21262266 0.23367149 0.22358938 0.19422908 0.13588746]\n",
      "[0.20483091 0.24535905 0.22289643 0.20298672 0.12392694]\n",
      "[0.2084316  0.22355096 0.22518164 0.20294471 0.13989106]\n",
      "\n",
      "[0.21515833 0.25627574 0.21566689 0.1896695  0.12322953]\n",
      "[0.22201861 0.23017275 0.21299037 0.1924831  0.14233515]\n",
      "[0.22445299 0.21772277 0.22435541 0.18528838 0.14818045]\n",
      "[0.22104214 0.23070513 0.21883635 0.19339964 0.13601676]\n",
      "[0.19676661 0.2526027  0.21903415 0.20721771 0.12437885]\n",
      "\n",
      "[0.20228599 0.24850874 0.23029636 0.19560495 0.123304  ]\n",
      "[0.2182514  0.22181627 0.21483353 0.19872467 0.1463741 ]\n",
      "[0.19929163 0.2923511  0.22015017 0.1808424  0.10736468]\n",
      "[0.21556234 0.19653839 0.22875148 0.20302154 0.15612626]\n",
      "[0.22088917 0.25919026 0.22333202 0.17557827 0.1210103 ]\n",
      "\n",
      "[0.22587304 0.23412181 0.22014292 0.18033369 0.13952859]\n",
      "[0.19374302 0.28093624 0.21820995 0.18939918 0.11771164]\n",
      "[0.23887664 0.20537795 0.22238486 0.17275184 0.1606087 ]\n",
      "[0.21140741 0.27057752 0.21198142 0.18626213 0.1197715 ]\n",
      "[0.22044031 0.22303139 0.22745937 0.19004355 0.13902539]\n",
      "\n",
      "[0.22322835 0.23330982 0.21863092 0.18879935 0.13603158]\n",
      "[0.20441993 0.24153438 0.22034948 0.20022811 0.13346808]\n",
      "[0.22479907 0.17664994 0.22958146 0.20128347 0.16768607]\n",
      "[0.20314576 0.26889065 0.22400092 0.182432   0.1215307 ]\n",
      "[0.22264437 0.20883307 0.22704281 0.18591653 0.15556322]\n",
      "\n",
      "[0.199008   0.26639315 0.21015826 0.20305236 0.12138826]\n",
      "[0.21561523 0.19152333 0.22673585 0.21264035 0.15348524]\n",
      "[0.2170282  0.28845745 0.21211468 0.16680405 0.11559568]\n",
      "[0.22135438 0.22084105 0.22300418 0.19301279 0.14178762]\n",
      "[0.22422238 0.2211677  0.22981626 0.18066835 0.14412531]\n",
      "\n",
      "[0.22514324 0.22562744 0.22113031 0.18151888 0.14658013]\n",
      "[0.20431758 0.26378307 0.2174363  0.19740634 0.1170566 ]\n",
      "[0.2368012  0.2082748  0.22363496 0.17244639 0.15884258]\n",
      "[0.21100225 0.24214143 0.21812712 0.19854775 0.13018143]\n",
      "[0.21134908 0.22557086 0.22715852 0.20109454 0.13482702]\n",
      "\n",
      "[0.22626516 0.18566026 0.22471897 0.18969189 0.1736637 ]\n",
      "[0.20991464 0.25449887 0.2258145  0.18517894 0.12459311]\n",
      "[0.22069362 0.24726129 0.22686833 0.1762484  0.12892841]\n",
      "[0.20973772 0.26549277 0.22524078 0.18327665 0.116252  ]\n",
      "[0.2118957  0.2509564  0.2239978  0.18862678 0.12452329]\n",
      "\n",
      "[0.2101295  0.25218526 0.22439171 0.18651812 0.12677541]\n",
      "[0.21472137 0.24148871 0.21937814 0.19266425 0.13174753]\n",
      "[0.23081045 0.23686497 0.21910216 0.17353235 0.13969003]\n",
      "[0.21773103 0.23980184 0.21924724 0.18883158 0.13438825]\n",
      "[0.21487397 0.22111182 0.23117046 0.1932943  0.1395495 ]\n",
      "\n",
      "[0.20635821 0.26365712 0.22638686 0.18581165 0.11778607]\n",
      "[0.21954602 0.20566517 0.22864355 0.20035584 0.14578947]\n",
      "[0.21879186 0.20527467 0.22829539 0.19490157 0.15273647]\n",
      "[0.21706122 0.23320886 0.22581665 0.18702163 0.1368916 ]\n",
      "[0.21573366 0.244135   0.22181426 0.18929574 0.12902136]\n",
      "\n",
      "[0.2261781  0.22176473 0.2223674  0.18294217 0.14674762]\n",
      "[0.2324129  0.2323909  0.2259613  0.16622347 0.14301144]\n",
      "[0.223666   0.22869422 0.21664909 0.18957786 0.14141288]\n",
      "[0.21635199 0.26867145 0.21707292 0.1781983  0.11970543]\n",
      "[0.21058594 0.26792142 0.21386865 0.18625474 0.12136929]\n",
      "\n",
      "[0.21900554 0.27957806 0.21360026 0.16925924 0.11855692]\n",
      "[0.21758318 0.22675206 0.22479232 0.19522502 0.13564745]\n",
      "[0.21561496 0.2226774  0.22310126 0.19989221 0.13871422]\n",
      "[0.22004586 0.22492802 0.22480641 0.18849096 0.14172868]\n",
      "[0.21800584 0.23378427 0.22219999 0.19081149 0.13519843]\n",
      "\n",
      "[0.22582246 0.21584377 0.2234601  0.18971166 0.145162  ]\n",
      "[0.21070836 0.21008064 0.23430248 0.20078133 0.14412715]\n",
      "[0.2212988  0.23203872 0.21632804 0.1932803  0.13705416]\n",
      "[0.21538837 0.2058449  0.21889244 0.21310675 0.1467675 ]\n",
      "[0.20461465 0.22141637 0.22226937 0.21192102 0.13977863]\n",
      "\n",
      "[0.23294911 0.20212069 0.22606169 0.18193467 0.15693387]\n",
      "[0.21150687 0.26200047 0.22102927 0.18700728 0.11845612]\n",
      "[0.21629757 0.24012014 0.23111641 0.17988157 0.13258427]\n",
      "[0.22124466 0.18187322 0.22651902 0.2029261  0.16743705]\n",
      "[0.20685111 0.237456   0.23659691 0.18889701 0.13019904]\n",
      "\n",
      "[0.20221321 0.27126384 0.21056388 0.19744538 0.11851368]\n",
      "[0.22074497 0.20906119 0.22518636 0.19735646 0.14765099]\n",
      "[0.21683772 0.24824983 0.22343048 0.18463497 0.12684692]\n",
      "[0.2153699  0.26178768 0.21690504 0.19567622 0.11026114]\n",
      "[0.21339647 0.2535094  0.22497979 0.18731694 0.12079737]\n",
      "\n",
      "[0.20888518 0.23938785 0.22481892 0.18889792 0.13801011]\n",
      "[0.21084663 0.2434294  0.21788357 0.19527963 0.1325607 ]\n",
      "[0.19798142 0.22632645 0.21650863 0.22195162 0.13723189]\n",
      "[0.21389768 0.22195657 0.22570361 0.19780852 0.1406336 ]\n",
      "[0.21610823 0.26128328 0.21765661 0.18000183 0.12495017]\n",
      "\n",
      "[0.21751736 0.22671232 0.22815238 0.18517886 0.14243908]\n",
      "[0.21485777 0.24024451 0.21763209 0.19247828 0.13478729]\n",
      "[0.21562827 0.18049154 0.22547483 0.21447591 0.16392948]\n",
      "[0.22050123 0.21901473 0.22775455 0.19005749 0.14267196]\n",
      "[0.21664326 0.22791618 0.22642538 0.19405943 0.13495572]\n",
      "\n",
      "[0.21997586 0.23556451 0.22273001 0.18846911 0.13326049]\n",
      "[0.22857894 0.24710472 0.21862654 0.17128956 0.13440038]\n",
      "[0.22354518 0.27384076 0.20999444 0.17527097 0.11734855]\n",
      "[0.2158409  0.26555115 0.21817598 0.17656213 0.12386981]\n",
      "[0.22100946 0.223006   0.22231913 0.19064821 0.14301725]\n",
      "\n",
      "[0.21935596 0.23372708 0.22242907 0.19026117 0.13422672]\n",
      "[0.2158592  0.25384578 0.21756692 0.18850681 0.12422136]\n",
      "[0.21127497 0.25586715 0.22241354 0.18654318 0.12390111]\n",
      "[0.21462943 0.2811868  0.21324658 0.17272075 0.11821648]\n",
      "[0.21446793 0.25435683 0.22383706 0.17793687 0.12940137]\n",
      "\n",
      "[0.21740802 0.23350064 0.22048783 0.19379099 0.13481241]\n",
      "[0.21698773 0.21502514 0.22099672 0.20662811 0.14036225]\n",
      "[0.21991357 0.23870303 0.22161433 0.18500236 0.13476668]\n",
      "[0.2248452  0.24521127 0.21925233 0.17841414 0.13227706]\n",
      "[0.22553477 0.1837293  0.2287521  0.19910344 0.16288035]\n",
      "\n",
      "[0.21145624 0.21007143 0.2236259  0.20776336 0.14708303]\n",
      "[0.2131517  0.26882592 0.21922174 0.1793756  0.11942507]\n",
      "[0.2189298  0.24709255 0.22234711 0.18060762 0.1310229 ]\n",
      "[0.21142189 0.24945758 0.22310078 0.19114785 0.12487195]\n",
      "[0.22721773 0.20876107 0.22293803 0.1893524  0.15173084]\n",
      "\n",
      "[0.22066924 0.24330792 0.2177999  0.18093833 0.13728462]\n",
      "[0.22160248 0.18160199 0.22776276 0.20228186 0.1667509 ]\n",
      "[0.20886233 0.23471731 0.22819668 0.1972215  0.13100214]\n",
      "[0.22450598 0.27442217 0.21211311 0.16538094 0.1235777 ]\n",
      "[0.21351853 0.23500505 0.22063439 0.19451156 0.13633038]\n",
      "\n",
      "[0.21146883 0.25213903 0.21988031 0.1932217  0.12329004]\n",
      "[0.21695477 0.23473683 0.21879505 0.19101058 0.1385028 ]\n",
      "[0.21795213 0.23501207 0.21694519 0.19321567 0.13687496]\n",
      "[0.21104777 0.22558662 0.2241704  0.20106569 0.13812959]\n",
      "[0.2181849  0.22392985 0.22505854 0.19271329 0.14011332]\n",
      "\n",
      "[0.20553717 0.24218756 0.22199723 0.20061585 0.12966223]\n",
      "[0.19415112 0.2763825  0.21920586 0.19098946 0.1192711 ]\n",
      "[0.21986268 0.22658177 0.22494718 0.18589027 0.14271818]\n",
      "[0.22777738 0.24734014 0.222863   0.17252272 0.12949671]\n",
      "[0.21947376 0.2042523  0.22399655 0.20045093 0.15182649]\n",
      "\n",
      "[0.20322298 0.2565502  0.21763058 0.20082937 0.12176695]\n",
      "[0.21698399 0.28130856 0.22111972 0.1686392  0.11194859]\n",
      "[0.22019549 0.22528169 0.22655222 0.19218563 0.13578492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2242059  0.21298023 0.22037363 0.19078308 0.1516572 ]\n",
      "[0.20396619 0.25523153 0.22252809 0.1957742  0.12249994]\n",
      "\n",
      "[0.21333747 0.20636319 0.22664948 0.20545466 0.14819522]\n",
      "[0.20151992 0.28966737 0.21825255 0.17881928 0.11174096]\n",
      "[0.21180518 0.22751749 0.22716019 0.19526762 0.13824949]\n",
      "[0.21623227 0.2227236  0.22569863 0.19739991 0.1379456 ]\n",
      "[0.22215946 0.23508789 0.21731377 0.182012   0.14342684]\n",
      "\n",
      "[0.2152606  0.24317682 0.22295639 0.1923544  0.12625185]\n",
      "[0.21425165 0.24918205 0.21947832 0.18672948 0.13035853]\n",
      "[0.21657386 0.20686142 0.22799271 0.20631416 0.14225787]\n",
      "[0.2198012  0.18842211 0.22526214 0.20336626 0.16314822]\n",
      "[0.21362345 0.24957725 0.2227816  0.18606177 0.12795587]\n",
      "\n",
      "[0.20879415 0.28129983 0.21555294 0.18973482 0.10461818]\n",
      "[0.20693961 0.2928036  0.20534395 0.18752693 0.10738587]\n",
      "[0.22713229 0.21451686 0.22164981 0.18781193 0.14888912]\n",
      "[0.22520345 0.18221782 0.22888982 0.19892257 0.16476624]\n",
      "[0.21625517 0.23595075 0.22402875 0.18682045 0.13694482]\n"
     ]
    }
   ],
   "source": [
    "for x in proba_seq:\n",
    "    print()\n",
    "    for xx in x:\n",
    "        print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  2 12 10  7]\n",
      " [ 3 13  7  8  2]\n",
      " [10 17  9 16  8]\n",
      " [19  9  7  1 14]\n",
      " [ 8 18  4 13 10]\n",
      " [12 13  3  4  6]\n",
      " [ 8  5  2 18 15]\n",
      " [ 3  2 12 17  1]\n",
      " [12  8 15 13 14]\n",
      " [18 10 12 17 11]\n",
      " [ 6 17 13  5 10]\n",
      " [ 5 10 13 17  3]\n",
      " [18 14 16 19  0]\n",
      " [15  1 14 19  6]\n",
      " [ 2  8  5  6  1]\n",
      " [ 6  5 16  7  9]\n",
      " [ 6  9  7 16  2]\n",
      " [ 3 10 13 11  4]\n",
      " [17  9  0 15  7]\n",
      " [16  8  9 11  5]\n",
      " [10 15 13  1  2]\n",
      " [ 6  3  8  4 19]\n",
      " [14 18 15  0 19]\n",
      " [19  2 16 18  1]\n",
      " [ 9 13 15  8  1]\n",
      " [14  9 15 11  2]\n",
      " [ 2  3  5 13 10]\n",
      " [13 11 14  5  0]\n",
      " [ 7  0 10 11  1]\n",
      " [15  8  3  5 13]\n",
      " [10 13  7  1 16]\n",
      " [ 4 15 13  5 12]]\n",
      "[[0 4 3 3 0]\n",
      " [4 4 4 4 0]\n",
      " [4 4 4 4 2]\n",
      " [4 4 0 1 0]\n",
      " [0 4 1 1 0]\n",
      " [1 0 0 0 3]\n",
      " [1 4 4 0 2]\n",
      " [0 3 3 1 2]\n",
      " [4 3 0 4 4]\n",
      " [0 2 0 3 0]\n",
      " [1 0 4 0 1]\n",
      " [0 0 3 0 3]\n",
      " [1 3 4 3 2]\n",
      " [0 0 4 4 0]\n",
      " [2 4 0 0 0]\n",
      " [0 1 0 0 1]\n",
      " [0 1 1 4 4]\n",
      " [0 2 1 1 0]\n",
      " [0 4 1 4 2]\n",
      " [0 1 2 3 4]\n",
      " [2 1 4 3 4]\n",
      " [3 2 2 3 1]\n",
      " [1 0 3 4 0]\n",
      " [2 0 4 0 4]\n",
      " [3 2 3 3 4]\n",
      " [4 3 4 4 0]\n",
      " [0 2 4 0 1]\n",
      " [4 3 4 0 3]\n",
      " [3 1 0 0 4]\n",
      " [0 2 0 4 0]\n",
      " [1 0 3 0 4]\n",
      " [4 2 0 2 3]]\n",
      "[0.6 0.4 0.4 0.6 0.6 0.6 0.8 0.8 0.6 0.6 0.6 0.4 0.8 0.4 0.6 0.4 0.6 0.6\n",
      " 0.8 1.  0.8 0.6 0.8 0.6 0.6 0.6 0.8 0.6 0.8 0.6 0.8 0.8]\n",
      "[[[0.29940635 0.15531819 0.12885839 0.16818833 0.24822867]\n",
      "  [0.29066718 0.15802214 0.12061422 0.16397749 0.26671895]\n",
      "  [0.30584615 0.14710905 0.1242556  0.15687343 0.26591572]\n",
      "  [0.29384047 0.15841277 0.13640746 0.17254631 0.23879299]\n",
      "  [0.29311162 0.15897028 0.12414666 0.16839603 0.25537544]]\n",
      "\n",
      " [[0.3115418  0.15206231 0.10264262 0.16647644 0.2672769 ]\n",
      "  [0.2878336  0.15553372 0.13764407 0.15485463 0.26413402]\n",
      "  [0.291286   0.15868403 0.13256843 0.16566075 0.25180084]\n",
      "  [0.2852245  0.16090241 0.1552881  0.1710754  0.22750957]\n",
      "  [0.2886458  0.15834898 0.12790413 0.1618141  0.26328698]]\n",
      "\n",
      " [[0.29533792 0.15615796 0.14269584 0.17015693 0.23565137]\n",
      "  [0.3083341  0.14735034 0.11249274 0.15973668 0.27208614]\n",
      "  [0.30847317 0.14808829 0.12027161 0.16211483 0.26105216]\n",
      "  [0.28670532 0.15569612 0.15828416 0.15913726 0.24017712]\n",
      "  [0.286899   0.16109456 0.15347227 0.17287254 0.22566174]]\n",
      "\n",
      " [[0.28985065 0.1616869  0.10880222 0.16865109 0.27100918]\n",
      "  [0.30739522 0.14869347 0.11115941 0.1617543  0.2709977 ]\n",
      "  [0.2942896  0.1573889  0.12193196 0.1656514  0.26073807]\n",
      "  [0.29492858 0.15446042 0.13126403 0.16101237 0.25833464]\n",
      "  [0.31844148 0.14493307 0.11344714 0.1635504  0.25962782]]\n",
      "\n",
      " [[0.2843967  0.16328765 0.1511821  0.17398739 0.22714612]\n",
      "  [0.297375   0.15056694 0.13649076 0.15533105 0.2602363 ]\n",
      "  [0.3011745  0.14920491 0.14205815 0.15827301 0.24928941]\n",
      "  [0.28830037 0.15707853 0.13143708 0.15778702 0.26539692]\n",
      "  [0.29327634 0.15761171 0.14100924 0.17067567 0.23742709]]\n",
      "\n",
      " [[0.30186155 0.14797887 0.13014628 0.15397449 0.2660387 ]\n",
      "  [0.2848     0.15838991 0.13460395 0.1564823  0.26572394]\n",
      "  [0.30678806 0.15284692 0.10328192 0.16443776 0.27264535]\n",
      "  [0.29872283 0.149079   0.14435403 0.15527947 0.25256464]\n",
      "  [0.2995661  0.14701167 0.16169906 0.15445565 0.23726761]]\n",
      "\n",
      " [[0.28882477 0.1601676  0.14910546 0.17232643 0.22957572]\n",
      "  [0.29887378 0.15296055 0.1298107  0.16508095 0.25327393]\n",
      "  [0.29177755 0.15708323 0.12261958 0.16268493 0.26583475]\n",
      "  [0.2998808  0.14850734 0.13598064 0.15380073 0.26183054]\n",
      "  [0.29341316 0.15234467 0.1446998  0.15541591 0.25412652]]\n",
      "\n",
      " [[0.30936217 0.15244707 0.1025347  0.1662348  0.26942125]\n",
      "  [0.2881688  0.15893471 0.1264935  0.16263558 0.26376748]\n",
      "  [0.30333278 0.14768158 0.13015302 0.15525247 0.26358014]\n",
      "  [0.30482087 0.14875485 0.114088   0.15901156 0.2733247 ]\n",
      "  [0.2920726  0.15447529 0.14339064 0.15987404 0.25018737]]\n",
      "\n",
      " [[0.30513087 0.14641647 0.12379607 0.15355866 0.2710979 ]\n",
      "  [0.28562763 0.16188712 0.14702632 0.1706193  0.23483966]\n",
      "  [0.29228815 0.15165913 0.14352342 0.15256327 0.25996605]\n",
      "  [0.28901508 0.15527576 0.12886801 0.15428375 0.27255738]\n",
      "  [0.3165298  0.14608146 0.11583104 0.16296507 0.25859255]]\n",
      "\n",
      " [[0.29948968 0.1492699  0.1364013  0.15462379 0.26021525]\n",
      "  [0.29511243 0.15697747 0.1381954  0.1708223  0.23889238]\n",
      "  [0.30705422 0.1458805  0.12576926 0.15543185 0.26586422]\n",
      "  [0.30759555 0.14793694 0.10817311 0.16014819 0.27614623]\n",
      "  [0.3008474  0.15365505 0.1302397  0.1661762  0.24908164]]\n",
      "\n",
      " [[0.3046952  0.1453439  0.152275   0.15502381 0.2426622 ]\n",
      "  [0.30641732 0.14808846 0.10528047 0.15886277 0.28135103]\n",
      "  [0.2894214  0.1560833  0.12717055 0.156004   0.2713208 ]\n",
      "  [0.2963365  0.15491797 0.12582032 0.16451056 0.25841472]\n",
      "  [0.29451323 0.15723757 0.13467747 0.16950674 0.24406503]]\n",
      "\n",
      " [[0.296208   0.15412256 0.12977819 0.1637776  0.25611368]\n",
      "  [0.2930991  0.157306   0.13904169 0.16865443 0.24189879]\n",
      "  [0.2883031  0.1558999  0.13196439 0.15493384 0.2688988 ]\n",
      "  [0.305536   0.14806114 0.10935745 0.15792944 0.279116  ]\n",
      "  [0.31004277 0.15116006 0.09977726 0.16433497 0.274685  ]]\n",
      "\n",
      " [[0.30097112 0.1478004  0.13638334 0.15332156 0.2615236 ]\n",
      "  [0.32267162 0.14493893 0.11482592 0.16589738 0.25166613]\n",
      "  [0.28812867 0.1551515  0.15255426 0.15858622 0.24557932]\n",
      "  [0.28962302 0.15941192 0.11445673 0.17048824 0.26602   ]\n",
      "  [0.30807838 0.14504893 0.14916395 0.15710744 0.24060135]]\n",
      "\n",
      " [[0.29436046 0.15043609 0.1410312  0.15262195 0.26155034]\n",
      "  [0.29554993 0.15336333 0.13184671 0.15952842 0.2597116 ]\n",
      "  [0.31878966 0.14478643 0.11356875 0.16300549 0.25984973]\n",
      "  [0.2890556  0.16038533 0.10956179 0.168636   0.2723613 ]\n",
      "  [0.3067116  0.145438   0.14869456 0.15584871 0.24330713]]\n",
      "\n",
      " [[0.2904584  0.15821595 0.12295508 0.16441444 0.2639561 ]\n",
      "  [0.2849121  0.16356224 0.14971732 0.1753927  0.22641563]\n",
      "  [0.29610232 0.15581432 0.129885   0.16790617 0.2502922 ]\n",
      "  [0.30728796 0.14628248 0.1539311  0.15939371 0.23310468]\n",
      "  [0.29383832 0.155247   0.13690668 0.1630691  0.25093898]]\n",
      "\n",
      " [[0.30825123 0.14447656 0.15222523 0.15596813 0.23907885]\n",
      "  [0.2979334  0.1539598  0.12733082 0.16463737 0.2561386 ]\n",
      "  [0.2876133  0.15572283 0.15128295 0.15852548 0.24685541]\n",
      "  [0.29551604 0.15623462 0.12517022 0.1649966  0.25808245]\n",
      "  [0.3087001  0.14785449 0.1147707  0.16119793 0.26747674]]\n",
      "\n",
      " [[0.30785346 0.14491823 0.15581843 0.15681225 0.23459767]\n",
      "  [0.3081973  0.14829767 0.11853765 0.16177233 0.26319507]\n",
      "  [0.29503092 0.15632384 0.12980498 0.16536255 0.25347775]\n",
      "  [0.2870303  0.15513249 0.15720372 0.15809871 0.24253476]\n",
      "  [0.29186636 0.156766   0.12390766 0.16213745 0.26532254]]\n",
      "\n",
      " [[0.31140083 0.15191628 0.10223547 0.16680272 0.2676447 ]\n",
      "  [0.29369402 0.15630297 0.14394565 0.16931693 0.23674032]\n",
      "  [0.2880168  0.15619397 0.1348317  0.15614711 0.26481047]\n",
      "  [0.29719907 0.15628783 0.1359603  0.16763453 0.24291822]\n",
      "  [0.30077305 0.14833911 0.14543942 0.15660042 0.24884802]]\n",
      "\n",
      " [[0.30907753 0.14561366 0.10508357 0.15635994 0.2838653 ]\n",
      "  [0.30937892 0.14688465 0.11272945 0.15922542 0.27178156]\n",
      "  [0.30888253 0.14413089 0.14481813 0.15467887 0.24748965]\n",
      "  [0.2957296  0.14929965 0.14166471 0.15130398 0.26200214]\n",
      "  [0.2965569  0.15479456 0.12332989 0.16234562 0.26297304]]\n",
      "\n",
      " [[0.28807357 0.15516774 0.15565795 0.15955393 0.2415468 ]\n",
      "  [0.28971317 0.15947592 0.15260284 0.17267464 0.22553347]\n",
      "  [0.31093463 0.14783257 0.11894062 0.16338742 0.25890476]\n",
      "  [0.302107   0.15293114 0.13413475 0.16652533 0.24430174]\n",
      "  [0.29969832 0.15260893 0.13312732 0.16560563 0.24895978]]\n",
      "\n",
      " [[0.2949639  0.15718889 0.13236952 0.16994148 0.24553622]\n",
      "  [0.29335234 0.15216546 0.13881788 0.15455066 0.2611136 ]\n",
      "  [0.2897168  0.1561086  0.12446116 0.15656288 0.2731505 ]\n",
      "  [0.29489064 0.15422861 0.1300037  0.16071223 0.2601649 ]\n",
      "  [0.29086843 0.1575815  0.11627144 0.16238588 0.2728927 ]]\n",
      "\n",
      " [[0.30056906 0.14652777 0.16563176 0.15601678 0.23125458]\n",
      "  [0.30993804 0.15259679 0.10542356 0.16683768 0.26520398]\n",
      "  [0.2836128  0.1617229  0.15870893 0.17214039 0.22381498]\n",
      "  [0.2991343  0.14881025 0.14949308 0.1565377  0.24602467]\n",
      "  [0.28346667 0.16032346 0.12338033 0.17162439 0.26120517]]\n",
      "\n",
      " [[0.32140496 0.14517382 0.11099331 0.16470766 0.25772023]\n",
      "  [0.30085343 0.14777361 0.13127515 0.15233997 0.26775792]\n",
      "  [0.29481825 0.15105483 0.13985847 0.15346652 0.26080185]\n",
      "  [0.308279   0.14511557 0.14417395 0.15620375 0.2462277 ]\n",
      "  [0.29032868 0.1597107  0.11039655 0.16817825 0.2713858 ]]\n",
      "\n",
      " [[0.2871435  0.15898392 0.11546353 0.17314267 0.26526636]\n",
      "  [0.29187104 0.15738426 0.12232386 0.16363537 0.2647855 ]\n",
      "  [0.28713194 0.15594023 0.15459754 0.15975164 0.24257873]\n",
      "  [0.30008957 0.14863081 0.13828357 0.15453051 0.25846565]\n",
      "  [0.29591823 0.15318085 0.13818146 0.16112071 0.2515987 ]]\n",
      "\n",
      " [[0.30764294 0.14835186 0.11292554 0.16072035 0.2703593 ]\n",
      "  [0.29036015 0.15497959 0.1264472  0.15497202 0.27324107]\n",
      "  [0.2937403  0.15140158 0.14087245 0.15327743 0.26070824]\n",
      "  [0.28707486 0.16158208 0.14483835 0.1713859  0.23511878]\n",
      "  [0.29553387 0.1530904  0.1322059  0.1590559  0.26011387]]\n",
      "\n",
      " [[0.3227712  0.14472605 0.11584377 0.16537063 0.25128832]\n",
      "  [0.30975786 0.14820302 0.11604984 0.16222174 0.26376763]\n",
      "  [0.29488266 0.15046242 0.14660856 0.15357158 0.25447476]\n",
      "  [0.30108184 0.15304424 0.13103133 0.1661028  0.24873982]\n",
      "  [0.29265946 0.15647589 0.12242692 0.1619946  0.2664432 ]]\n",
      "\n",
      " [[0.28868157 0.15869719 0.12282927 0.1626012  0.2671908 ]\n",
      "  [0.3115392  0.1522082  0.09827042 0.16722402 0.27075815]\n",
      "  [0.29586786 0.15483284 0.12962495 0.1657155  0.25395882]\n",
      "  [0.28746447 0.15701793 0.13122167 0.15663956 0.26765633]\n",
      "  [0.2924093  0.15827817 0.13886918 0.17067264 0.23977062]]\n",
      "\n",
      " [[0.29133084 0.15485837 0.12658036 0.15608028 0.27115008]\n",
      "  [0.30168    0.1533671  0.12734692 0.16565925 0.25194672]\n",
      "  [0.322386   0.14545272 0.11159353 0.16624944 0.25431827]\n",
      "  [0.2983084  0.15350395 0.12692563 0.16417657 0.25708538]\n",
      "  [0.3080583  0.1453016  0.14610164 0.1570822  0.24345623]]\n",
      "\n",
      " [[0.2959742  0.15687558 0.1212613  0.1671154  0.25877357]\n",
      "  [0.30870068 0.14557494 0.14353944 0.15898782 0.24319708]\n",
      "  [0.29666483 0.15675561 0.13370526 0.17165482 0.24121937]\n",
      "  [0.3021804  0.15359022 0.1250604  0.16759755 0.25157142]\n",
      "  [0.29624963 0.15471919 0.13033545 0.16348772 0.25520805]]\n",
      "\n",
      " [[0.2916397  0.1520353  0.14655174 0.15263358 0.25713962]\n",
      "  [0.28612596 0.16054821 0.1510552  0.1696324  0.23263827]\n",
      "  [0.31223464 0.15089397 0.09921525 0.16457953 0.2730766 ]\n",
      "  [0.29747015 0.1524315  0.13104606 0.16206056 0.25699174]\n",
      "  [0.28899443 0.15465474 0.13195877 0.15347813 0.27091396]]\n",
      "\n",
      " [[0.29565975 0.15682788 0.13349131 0.17013077 0.24389026]\n",
      "  [0.29038817 0.15601525 0.12570855 0.15701456 0.27087346]\n",
      "  [0.29500186 0.15680963 0.12217952 0.16548942 0.26051956]\n",
      "  [0.29551816 0.15410723 0.13128181 0.16114542 0.25794736]\n",
      "  [0.28703618 0.1570433  0.14702798 0.15993908 0.2489534 ]]\n",
      "\n",
      " [[0.30124444 0.14867572 0.13503471 0.15564233 0.2594028 ]\n",
      "  [0.29151598 0.15274087 0.13961513 0.15330113 0.2628269 ]\n",
      "  [0.2880452  0.156374   0.12520915 0.1550399  0.2753318 ]\n",
      "  [0.2953575  0.15514238 0.12449721 0.1639008  0.26110223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [0.30450714 0.14674391 0.12129466 0.1536208  0.27383348]]]\n"
     ]
    }
   ],
   "source": [
    "print(state_seq)\n",
    "print(action_seq)\n",
    "print(reward_seq)\n",
    "print(proba_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0]\n",
      " [0 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    inputs = tf.constant(np.array([[0,0], [0,1], [0,2], [1,1], [1,2], [1,3]]))\n",
    "    \n",
    "    updates = tf.constant(np.array([1,1,1] +[2,2,2]))\n",
    "    \n",
    "    scatter = tf.scatter_nd(inputs, updates, shape = [2,4])\n",
    "    \n",
    "    print(sess.run(scatter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 2 2 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.reshape(tf.tile(tf.expand_dims([1, 2, 3], -1),  [1, 3]), [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3],\n",
       "       [0, 0, 0],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([[[0,1,2], [1,2,3]], [[0,0,0], [1,1,1]]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 0.]\n",
      " [0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((2,4))\n",
    "x[(np.repeat(np.arange(2), 3), np.array([[0,1,2], [1,2,3]]).ravel())] = 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1],\n",
       "       [0, 1, 2, 1, 2, 3]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((np.repeat(np.arange(2), 3), np.array([[0,1,2], [1,2,3]]).ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 1, 1, 1, 1], dtype=int64),\n",
       " array([0, 1, 2, 3, 0, 1, 2, 3], dtype=int64))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.ones((2,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
