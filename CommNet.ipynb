{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Romain Zimmer\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNet:\n",
    "    \n",
    "    def __init__(self, sess, N, J, embedding_size = 128, lr = 1e-3, training_mode = 'supervised', alpha = 0.03):\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        \n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        if training_mode == 'supervised':\n",
    "            self.build_supervised()\n",
    "            with tf.variable_scope('Supervised_optimizer'):\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "                \n",
    "        elif training_mode == 'reinforce':\n",
    "            self.alpha = 0.03\n",
    "            self.build_reinforce()\n",
    "            with tf.variable_scope('Reinforce_optimizer'):\n",
    "                self.train_op =  tf.train.RMSPropOptimizer(lr).minimize(self.reinforce_loss)\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError(\"Unknown training mode: %s\" % training_mode))\n",
    "        \n",
    "        print(\"All variables\")\n",
    "        for var in tf.global_variables():\n",
    "            print(var)\n",
    "            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            self.identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            self.embedded_identities = tf.nn.embedding_lookup(self.identity_embeddings, inputs)\n",
    "        \n",
    "            \n",
    "        return tf.unstack(self.embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None and c is not None:\n",
    "            \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            elif h0 is not None and c is None: \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, h0], axis = 1)\n",
    "                \n",
    "            elif c is not None and h0 is None:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = h\n",
    "                \n",
    "                \n",
    "            b2 = tf.get_variable('b2', shape = (1, self.embedding_size))\n",
    "            \n",
    "            dense1 =tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1) + b1)\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2) + b2)\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1, self.J))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "        \n",
    "            return policy_logit\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_actions(self, log_proba):\n",
    "        \n",
    "        action = tf.multinomial(log_proba, num_samples = 1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        self.h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        self.layers = {'h0_seq': h0_seq, 'h1_seq': h1_seq, 'c1_seq':c1_seq, 'h2_seq': self.h2_seq}\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in self.h2_seq]\n",
    "        \n",
    "        self.log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.policy_logit_seq]\n",
    "        \n",
    "        self.action_seq = [self.sample_actions(log_proba) for log_proba in self.log_proba_seq]\n",
    "        \n",
    "        self.one_hot_action_seq = [tf.one_hot(action, depth = self.J) for action in self.action_seq]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_supervised(self):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_mean(supervised_loss_seq)\n",
    "        \n",
    "        \n",
    "    def supervised_train(self, X, y, val_X, val_y, env, batch_size = 32, epochs = 1):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            np.random.shuffle(data_inds)\n",
    "            supervised_loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm_notebook(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                _, supervised_loss, one_hot_action_seq = sess.run([self.train_op, self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "                supervised_loss_sum += supervised_loss * batch_size\n",
    "                reward_sum += env.get_reward(one_hot_action_seq)\n",
    "            \n",
    "            print(\"loss = %f\" % (supervised_loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            val_supervised_loss, val_one_hot_action_seq = sess.run([self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss))\n",
    "            print('val reward = %f' % (env.get_reward(val_one_hot_action_seq) / val_n))\n",
    "            \n",
    "    def build_baseline(self, h):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        with tf.variable_scope('Baseline', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  1))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1,))\n",
    "            \n",
    "            \n",
    "            baseline = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "            \n",
    "            return baseline\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        self.indices = tf.placeholder(tf.int32, shape = (None, 2))\n",
    "        \n",
    "        self.shape = tf.placeholder(tf.int32, shape =(2,))\n",
    "        \n",
    "        self.baselines = tf.concat([self.build_baseline(h2) for h2 in self.h2_seq], axis = 1)\n",
    "        self.scattered_baselines = tf.scatter_nd(self.indices, tf.reshape(self.baselines, [-1]), shape = self.shape)\n",
    "                    \n",
    "        self.repeated_reward_values = tf.placeholder(tf.float32, shape = (None,))\n",
    "        self.scattered_reward_values = tf.scatter_nd(self.indices, self.repeated_reward_values, shape = self.shape)\n",
    "        self.scattered_reward_values_cumsum = tf.cumsum(self.scattered_reward_values, axis = 0, reverse = True)\n",
    "        \n",
    "        self.baseline_values =  tf.placeholder(tf.float32, shape = (None, self.J))\n",
    "        self.scattered_baseline_values = tf.scatter_nd(self.indices, tf.reshape(self.baseline_values, [-1]), shape = self.shape)\n",
    "        \n",
    "        \n",
    "        self.action_taken = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_action_taken = tf.unstack(self.action_taken, axis = 1)\n",
    "        \n",
    "        self.neg_log_p = tf.transpose(tf.concat([[tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_action_taken[j],\n",
    "                                                    logits=self.policy_logit_seq[j])] for j in range(self.J)], axis = 0))\n",
    "        \n",
    "        \n",
    "        self.scattered_neg_log_p = tf.scatter_nd(self.indices, tf.reshape(self.neg_log_p, [-1]), shape = self.shape)\n",
    "        \n",
    "        #surrogate loss (- dtheta)\n",
    "        self.reinforce_loss = tf.reduce_sum(tf.multiply(self.scattered_neg_log_p, self.scattered_reward_values_cumsum - self.scattered_baseline_values))\n",
    "        print(tf.multiply(self.scattered_neg_log_p, self.scattered_reward_values_cumsum - self.scattered_baseline_values))\n",
    "        self.reinforce_loss += self.alpha * tf.reduce_sum(tf.square(self.scattered_reward_values_cumsum - self.scattered_baselines))\n",
    "        print(tf.square(self.scattered_reward_values_cumsum - self.scattered_baselines))\n",
    "        self.reinforce_loss /= self.J\n",
    "        \n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        action_seq, baselines= self.sess.run([self.action_seq, self.baselines], {self.inputs: [state]})\n",
    "        \n",
    "        return [a[0,0] for a in action_seq], baselines\n",
    "    \n",
    "    def reinforce_train(self, env, n_episodes, T):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        \n",
    "        history = {'reward' : [],  'loss': []}    \n",
    "        \n",
    "        for _ in tqdm_notebook(range(n_episodes), \"REINFORCE\"):\n",
    "            \n",
    "            \n",
    "            state_seq, action_seq, reward_seq, baseline_seq = policy_rollout(T, env, self)\n",
    "            episode_len = reward_seq.shape[0]\n",
    "            \n",
    "            history['reward'].append(np.mean(reward_seq))\n",
    "            \n",
    "            repeated_t = np.repeat(np.arange(episode_len), self.J)\n",
    "            \n",
    "            indices = np.vstack([repeated_t, state_seq.ravel()]) .T\n",
    "                \n",
    "            feed_dict = {}\n",
    "            feed_dict[self.inputs] = state_seq\n",
    "            feed_dict[self.indices] = indices\n",
    "            feed_dict[self.shape] = [episode_len, self.N]\n",
    "            feed_dict[self.repeated_reward_values] = np.repeat(reward_seq, self.J)\n",
    "            feed_dict[self.baseline_values] = baseline_seq\n",
    "            feed_dict[self.action_taken] = action_seq\n",
    "            \n",
    "            _, loss = self.sess.run([self.train_op, self.reinforce_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "            \n",
    "        return history\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeverEnv:\n",
    "    \n",
    "    def __init__(self, N, J):\n",
    "        \n",
    "        self.J = J\n",
    "        self.N = N\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return state, terminal_state\n",
    "    \n",
    "    def get_reward(self, one_hot_action_seq):        \n",
    "        \n",
    "        reward = np.sum(np.sum(one_hot_action_seq, axis = 0) > 0) /self.J\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        \n",
    "        next_state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        one_hot_action_seq = np.zeros((self.J, self.J))\n",
    "        one_hot_action_seq[range(self.J), action] = 1\n",
    "        reward = self.get_reward(one_hot_action_seq)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return next_state, reward, terminal_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation for supervised learning\n",
    "def generate_data(n, N, J):\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    y= np.empty((n,J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.random.choice(N, size = J, replace = False)\n",
    "        sorted_args = np.argsort(X[i])\n",
    "        y[i] = np.argsort(sorted_args)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode generation for reinforcement learning\n",
    "def policy_rollout(T, env, agent):\n",
    "    \n",
    "    state_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    baseline_seq = []\n",
    "    \n",
    "    \n",
    "    state, terminal_state = env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while not terminal_state and t < T:\n",
    "        t +=1\n",
    "        \n",
    "        state_seq.append(state)\n",
    "        action, baseline = agent.take_action(state)\n",
    "        \n",
    "        state, reward, terminal_state = env.step(state, action)\n",
    "        \n",
    "        \n",
    "        action_seq.append(action)\n",
    "        reward_seq.append(reward)\n",
    "        baseline_seq.append(baseline)\n",
    "        \n",
    "    return np.array(state_seq), np.array(action_seq), np.array(reward_seq), np.squeeze(np.array(baseline_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "J = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size * 1000\n",
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(500, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables\n",
      "<tf.Variable 'Encoder/identity_embeddings:0' shape=(10, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b2:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b2:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/W:0' shape=(128, 3) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/b:0' shape=(1, 3) dtype=float32_ref>\n",
      "<tf.Variable 'Baseline/W:0' shape=(128, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Baseline/b:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/RMSProp:0' shape=(10, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/RMSProp_1:0' shape=(10, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/RMSProp:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/RMSProp_1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/RMSProp:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/RMSProp_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/RMSProp:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/RMSProp_1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/RMSProp:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/RMSProp_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/RMSProp:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/RMSProp_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/RMSProp:0' shape=(128, 3) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/RMSProp_1:0' shape=(128, 3) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/b/RMSProp:0' shape=(1, 3) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/b/RMSProp_1:0' shape=(1, 3) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/W/RMSProp:0' shape=(128, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/W/RMSProp_1:0' shape=(128, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/b/RMSProp:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/b/RMSProp_1:0' shape=(1,) dtype=float32_ref>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161fb513df344c619b8ee09656809a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='REINFORCE', max=1, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNet(sess, N, J, lr = 1e-3, embedding_size= 128, training_mode = 'reinforce')\n",
    "    env = LeverEnv(N, J)\n",
    "    \n",
    "    #commNet.supervised_train(X, y, val_X, val_y, env, batch_size = batch_size, epochs = 1)\n",
    "    #rv = sess.run([commNet.embedded_identities, commNet.identity_embeddings, commNet.layers], feed_dict = {commNet.inputs: val_X[0:2], commNet.targets: val_y[0:2]})\n",
    "    \n",
    "    history = commNet.reinforce_train(env, n_episodes = 1, T =64)\n",
    "    state_seq, action_seq, reward_seq, baseline_seq  = policy_rollout(T = 3, env = env, agent = commNet)\n",
    "    \n",
    "    episode_len = reward_seq.shape[0]\n",
    "    repeated_t = np.repeat(np.arange(episode_len), commNet.J)\n",
    "    indices = np.vstack([repeated_t, state_seq.ravel()]) .T\n",
    "\n",
    "    feed_dict = {}\n",
    "    feed_dict[commNet.inputs] = state_seq\n",
    "    feed_dict[commNet.indices] = indices\n",
    "    feed_dict[commNet.shape] = [episode_len, commNet.N]\n",
    "    feed_dict[commNet.repeated_reward_values] = np.repeat(reward_seq, commNet.J)\n",
    "    feed_dict[commNet.baseline_values] = baseline_seq\n",
    "    feed_dict[commNet.action_taken] = action_seq\n",
    "\n",
    "    rv = commNet.sess.run([commNet.scattered_baselines, commNet.scattered_reward_values_cumsum,\n",
    "                        commNet.scattered_baseline_values, commNet.scattered_neg_log_p, commNet.reinforce_loss, commNet.policy_logit_seq], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 7],\n",
       "       [7, 3, 5],\n",
       "       [3, 6, 7]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14, -0.21, -0.28],\n",
       "       [-0.24, -0.18, -0.28],\n",
       "       [-0.2 , -0.29, -0.25]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(baseline_seq, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21  0.   -0.14  0.    0.    0.    0.   -0.28  0.    0.  ]\n",
      "[ 0.    0.    0.   -0.18  0.   -0.28  0.   -0.24  0.    0.  ]\n",
      "[ 0.    0.    0.   -0.2   0.    0.   -0.29 -0.25  0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "for x in rv[0]:\n",
    "    print(np.round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21  0.   -0.14  0.    0.    0.    0.   -0.28  0.    0.  ]\n",
      "[ 0.    0.    0.   -0.18  0.   -0.28  0.   -0.24  0.    0.  ]\n",
      "[ 0.    0.    0.   -0.2   0.    0.   -0.29 -0.25  0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "for x in rv[2]:\n",
    "    print(np.round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67 0.67 0.67]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(reward_seq, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67 0.   0.67 1.33 0.   0.67 0.67 2.   0.   0.  ]\n",
      "[0.   0.   0.   1.33 0.   0.67 0.67 1.33 0.   0.  ]\n",
      "[0.   0.   0.   0.67 0.   0.   0.67 0.67 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "for x in rv[1]:\n",
    "    print(np.round(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2],\n",
       "       [2, 0, 2],\n",
       "       [1, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.974 0.    1.097 0.    0.    0.    0.    0.775 0.    0.   ]\n",
      "[0.    0.    0.    1.083 0.    0.836 0.    0.742 0.    0.   ]\n",
      "[0.    0.    0.    1.484 0.    0.    0.848 0.72  0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "for x in rv[3]:\n",
    "    print(np.round(x,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.0972534 , 1.6044937 , 0.76524043],\n",
       "        [1.0889457 , 1.6756624 , 0.7418233 ],\n",
       "        [1.0931096 , 1.4844712 , 0.8250679 ]],\n",
       "\n",
       "       [[0.97443944, 1.5820056 , 0.8745882 ],\n",
       "        [1.0834335 , 1.5042886 , 0.82236075],\n",
       "        [1.0432738 , 1.5164404 , 0.8481443 ]],\n",
       "\n",
       "       [[1.0961562 , 1.5833855 , 0.7752948 ],\n",
       "        [1.0126504 , 1.5935549 , 0.83576405],\n",
       "        [1.1163138 , 1.6840987 , 0.719689  ]]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- np.log(np.exp(rv[5])/ np.sum(np.exp(rv[5]), axis = 2, keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.463704"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22bd3c33390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22bd6a1dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22bd6a9d320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = 30\n",
    "plt.plot(np.convolve(history['reward'], np.ones(W), mode= 'valid')/W)\n",
    "plt.show()\n",
    "plt.plot(history['reward'], '*')\n",
    "plt.show()\n",
    "plt.plot(history['loss'], '*')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
