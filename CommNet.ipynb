{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Romain Zimmer\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNet:\n",
    "    \n",
    "    def __init__(self, sess, N, J, embedding_size = 128, lr = 1e-3, training_mode = 'supervised'):\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        \n",
    "        if training_mode == 'supervised':\n",
    "            self.build_supervised()\n",
    "            with tf.variable_scope('Supervised_optimizer'):\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "                \n",
    "        elif training_mode == 'reinforce':\n",
    "            self.build_reinforce()\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError(\"Unknown training mode: %s\" % training_mode))\n",
    "        \n",
    "        print(\"All variables\")\n",
    "        for var in tf.global_variables():\n",
    "            print(var)\n",
    "            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            embedded_identities = tf.nn.embedding_lookup(identity_embeddings, inputs)\n",
    "            \n",
    "        return tf.unstack(embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None:\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            else:\n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "            \n",
    "            W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "            \n",
    "            dense1 =  tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1))\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2))\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            policy_logits = tf.einsum(\"ij,jk->ik\", h, W)\n",
    "        \n",
    "            return policy_logits\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_actions(self, policy_logit):\n",
    "        \n",
    "        \n",
    "        action = tf.multinomial(policy_logit, num_samples = 1)\n",
    "        \n",
    "        return action      \n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in h2_seq]\n",
    "        \n",
    "        self.actions = [self.sample_actions(policy_logit) for policy_logit in self.policy_logit_seq]\n",
    "        \n",
    "        one_hot_actions = [tf.one_hot(tf.reshape(action, [-1]), depth = self.J) for action in self.actions]\n",
    "        \n",
    "        self.reward = tf.reduce_sum(tf.count_nonzero(tf.add_n(one_hot_actions), axis = 1) / self.J)\n",
    "        \n",
    "    def build_supervised(self):\n",
    "        \n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_sum(tf.add_n(supervised_loss_seq))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def supervised_train(self, X, y, val_X, val_y, batch_size = 32, epochs = 1):\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            np.random.shuffle(data_inds)\n",
    "            supervised_loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                _, supervised_loss, reward = sess.run([self.train_op, self.supervised_loss, self.reward], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "                supervised_loss_sum += supervised_loss\n",
    "                reward_sum += reward\n",
    "            \n",
    "            print(\"loss = %f\" % (supervised_loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            val_supervised_loss, val_reward = sess.run([self.supervised_loss, self.reward], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss / val_n))\n",
    "            print('val reward = %f' % (val_reward / val_n))\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        log_p_seq = [tf.log(tf.nn.softmax(policy_logit)) for policy_logit in self.policy_logit_seq]\n",
    "        \n",
    "        self.advantage = tf.placeholder(tf.float32, shape = (None, self.J))\n",
    "        unstacked_advantage = tf.unstack(self.advantage, axis = 1)\n",
    "        \n",
    "        self.actions_taken = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_actions_taken = tf.unstack(self.actions_taken, axis = 1)\n",
    "        \n",
    "        print(log_p_seq[0])\n",
    "        print(unstacked_actions_taken[0])\n",
    "        \n",
    "        self.actions_taken_p_seq = [tf.gather(log_p_seq[j], unstacked_actions_taken[j], axis = 1) for j in range(self.J)]\n",
    "        \n",
    "        #surrogate loss\n",
    "        self.reinforce_loss =  - tf.add_n([tf.reduce_sum(tf.multiply(unstacked_advantage[j], self.actions_taken_p_seq[j])) for j in range(self.J)])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, N, J):\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.sort(np.random.choice(N, size = J, replace = False))\n",
    "        \n",
    "    y = np.tile([j for j in range(J)], (n,1))\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "J = 5\n",
    "batch_size = 32\n",
    "n = batch_size * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(1024, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables\n",
      "<tf.Variable 'Encoder/identity_embeddings:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/W:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/beta1_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/beta2_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Encoder/identity_embeddings/Adam:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Encoder/identity_embeddings/Adam_1:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_1/W1/Adam:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_1/W1/Adam_1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_1/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_1/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_2/W1/Adam:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_2/W1/Adam_1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_2/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Comm_step_2/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Decoder/W/Adam:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Supervised_optimizer/Decoder/W/Adam_1:0' shape=(128, 5) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 8.050602\n",
      "reward = 0.637500\n",
      "\n",
      "val loss = 8.048738\n",
      "val reward = 0.673828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 47.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 8.038939\n",
      "reward = 0.668750\n",
      "\n",
      "val loss = 8.045919\n",
      "val reward = 0.667383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 76.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 8.027385\n",
      "reward = 0.631250\n",
      "\n",
      "val loss = 8.043117\n",
      "val reward = 0.677734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 71.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 8.015928\n",
      "reward = 0.643750\n",
      "\n",
      "val loss = 8.040318\n",
      "val reward = 0.675000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 8.004602\n",
      "reward = 0.650000\n",
      "\n",
      "val loss = 8.037531\n",
      "val reward = 0.674609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 7.993379\n",
      "reward = 0.618750\n",
      "\n",
      "val loss = 8.034752\n",
      "val reward = 0.675586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 7.982247\n",
      "reward = 0.693750\n",
      "\n",
      "val loss = 8.031969\n",
      "val reward = 0.675195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 52.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 7.971165\n",
      "reward = 0.706250\n",
      "\n",
      "val loss = 8.029185\n",
      "val reward = 0.667773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 7.960152\n",
      "reward = 0.681250\n",
      "\n",
      "val loss = 8.026405\n",
      "val reward = 0.658789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 7.949202\n",
      "reward = 0.693750\n",
      "\n",
      "val loss = 8.023636\n",
      "val reward = 0.677344\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    commNet = CommNet(sess, N, J, lr = 1e-4, training_mode = 'supervised')\n",
    "    commNet.supervised_train(X, y, val_X, val_y, batch_size = batch_size, epochs = 10)\n",
    "    \n",
    "    #print(sess.run(commNet.actions_taken_p_seq, feed_dict={commNet.inputs: X, commNet.advantage: np.ones((1,5)), commNet.actions_taken: np.ones((1,5))}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
