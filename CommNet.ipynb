{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Romain Zimmer\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNetLever:\n",
    "    \n",
    "    def __init__(self, sess, N, J, embedding_size = 128, lr = 1e-3, training_mode = 'supervised', alpha = 0.03):\n",
    "        \n",
    "        '''\n",
    "        - N: total number of agents\n",
    "        - J: number of levers (and agents randomly selected at each step)\n",
    "        - embedding_size: dimension of the hidden layers \n",
    "        - lr: learning rate \n",
    "        - training_mode: 'supervised' or 'reinforce'\n",
    "        - alpha: paramater used by reinforce training mode to balance reward and baseline loss\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        \n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        if training_mode == 'supervised':\n",
    "            self.build_supervised()\n",
    "            with tf.variable_scope('Supervised_optimizer'):\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "                \n",
    "        elif training_mode == 'reinforce':\n",
    "            self.alpha = 0.03\n",
    "            self.build_reinforce()\n",
    "            with tf.variable_scope('Reinforce_optimizer'):\n",
    "                self.train_op =  tf.train.RMSPropOptimizer(lr).minimize(self.reinforce_loss)\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError(\"Unknown training mode: %s\" % training_mode))            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            self.identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            self.embedded_identities = tf.nn.embedding_lookup(self.identity_embeddings, inputs)\n",
    "        \n",
    "            \n",
    "        return tf.unstack(self.embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None and c is not None:\n",
    "            \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            elif h0 is not None and c is None: \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, h0], axis = 1)\n",
    "                \n",
    "            elif c is not None and h0 is None:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = h\n",
    "                \n",
    "                \n",
    "            b2 = tf.get_variable('b2', shape = (1, self.embedding_size))\n",
    "            \n",
    "            dense1 =tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1) + b1)\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2) + b2)\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1, self.J))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "        \n",
    "            return policy_logit\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        # mean of hidden layers \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_actions(self, log_proba):\n",
    "        \n",
    "        action = tf.multinomial(log_proba, num_samples = 1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        self.h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        # can be used to check values of hidden states \n",
    "        self.hidden_layers = {'h0_seq': h0_seq, 'h1_seq': h1_seq, 'c1_seq':c1_seq, 'h2_seq': self.h2_seq}\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in self.h2_seq]\n",
    "        self.log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.policy_logit_seq]\n",
    "        self.action_seq = [self.sample_actions(log_proba) for log_proba in self.log_proba_seq]\n",
    "        self.one_hot_action_seq = [tf.one_hot(action, depth = self.J) for action in self.action_seq]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_supervised(self):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_mean(supervised_loss_seq)\n",
    "        \n",
    "        \n",
    "    def supervised_train(self, X, y, val_X, val_y, env, batch_size = 32, epochs = 1):\n",
    "    \n",
    "            \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            # shuffle data for each epoch\n",
    "            np.random.shuffle(data_inds)\n",
    "            \n",
    "            supervised_loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm_notebook(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                \n",
    "                # select batch data\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                \n",
    "                # train on batch\n",
    "                _, supervised_loss, one_hot_action_seq = sess.run([self.train_op, self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "               \n",
    "                # keep track of the loss and reward\n",
    "                supervised_loss_sum += supervised_loss * batch_size\n",
    "                reward_sum += env.get_reward(one_hot_action_seq)\n",
    "            \n",
    "            print(\"loss = %f\" % (supervised_loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            # eval loss and reward on validation set\n",
    "            val_supervised_loss, val_one_hot_action_seq = sess.run([self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss))\n",
    "            print('val reward = %f' % (env.get_reward(val_one_hot_action_seq) / val_n))\n",
    "            \n",
    "    def build_baseline(self, h):\n",
    "        \n",
    "        '''state specific baseline for reinforce training mode is given by a simple FC layer\n",
    "        connected to the last hidden layer of the controler\n",
    "        '''\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        with tf.variable_scope('Baseline', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  1))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1,))\n",
    "            \n",
    "            \n",
    "            baseline = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "            \n",
    "            return baseline\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        # only used for scattering \n",
    "        self.indices = tf.placeholder(tf.int32, shape = (None, 2))\n",
    "        self.shape = tf.placeholder(tf.int32, shape =(2,))\n",
    "        \n",
    "        # baseline tensors\n",
    "        self.baselines = tf.concat([self.build_baseline(h2) for h2 in self.h2_seq], axis = 1)\n",
    "        self.scattered_baselines = tf.scatter_nd(self.indices, tf.reshape(self.baselines, [-1]), shape = self.shape)\n",
    "                    \n",
    "        # reward values\n",
    "        self.repeated_reward_values = tf.placeholder(tf.float32, shape = (None,))\n",
    "        self.scattered_reward_values = tf.scatter_nd(self.indices, self.repeated_reward_values, shape = self.shape)\n",
    "        self.scattered_reward_values_cumsum = tf.cumsum(self.scattered_reward_values, axis = 0, reverse = True)\n",
    "        \n",
    "        # baseline values\n",
    "        self.baseline_values =  tf.placeholder(tf.float32, shape = (None, self.J))\n",
    "        self.scattered_baseline_values = tf.scatter_nd(self.indices, tf.reshape(self.baseline_values, [-1]), shape = self.shape)\n",
    "        \n",
    "        # actions that have been taken\n",
    "        self.action_taken = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_action_taken = tf.unstack(self.action_taken, axis = 1)\n",
    "        \n",
    "        # neg log proba of taken actions\n",
    "        self.neg_log_p = tf.transpose(tf.concat([[tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_action_taken[j],\n",
    "                                                    logits=self.policy_logit_seq[j])] for j in range(self.J)], axis = 0))\n",
    "        self.scattered_neg_log_p = tf.scatter_nd(self.indices, tf.reshape(self.neg_log_p, [-1]), shape = self.shape)\n",
    "        \n",
    "        #surrogate loss (- dtheta)\n",
    "        self.reinforce_loss = tf.reduce_sum(tf.multiply(self.scattered_neg_log_p, self.scattered_reward_values_cumsum - self.scattered_baseline_values))\n",
    "        self.reinforce_loss += self.alpha * tf.reduce_sum(tf.square(self.scattered_reward_values_cumsum - self.scattered_baselines))\n",
    "        self.reinforce_loss /= self.J\n",
    "        \n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        action_seq, baselines= self.sess.run([self.action_seq, self.baselines], {self.inputs: [state]})\n",
    "        \n",
    "        return [a[0,0] for a in action_seq], baselines\n",
    "    \n",
    "    def reinforce_train(self, env, n_episodes, T):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        history = {'reward' : [],  'loss': []}    \n",
    "        \n",
    "        for _ in tqdm_notebook(range(n_episodes), \"REINFORCE\"):\n",
    "            \n",
    "            \n",
    "            state_seq, action_seq, reward_seq, baseline_seq = policy_rollout(T, env, self)\n",
    "            episode_len = reward_seq.shape[0]\n",
    "            \n",
    "            history['reward'].append(np.mean(reward_seq))\n",
    "            \n",
    "            repeated_t = np.repeat(np.arange(episode_len), self.J)\n",
    "            \n",
    "            indices = np.vstack([repeated_t, state_seq.ravel()]) .T\n",
    "                \n",
    "            feed_dict = {}\n",
    "            feed_dict[self.inputs] = state_seq\n",
    "            feed_dict[self.indices] = indices\n",
    "            feed_dict[self.shape] = [episode_len, self.N]\n",
    "            feed_dict[self.repeated_reward_values] = np.repeat(reward_seq, self.J)\n",
    "            feed_dict[self.baseline_values] = baseline_seq\n",
    "            feed_dict[self.action_taken] = action_seq\n",
    "            \n",
    "            _, loss = self.sess.run([self.train_op, self.reinforce_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "            \n",
    "        return history\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeverEnv:\n",
    "    \n",
    "    def __init__(self, N, J):\n",
    "        \n",
    "        self.J = J\n",
    "        self.N = N\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return state, terminal_state\n",
    "    \n",
    "    def get_reward(self, one_hot_action_seq):        \n",
    "        \n",
    "        reward = np.sum(np.sum(one_hot_action_seq, axis = 0) > 0) /self.J\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        \n",
    "        next_state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        one_hot_action_seq = np.zeros((self.J, self.J))\n",
    "        one_hot_action_seq[range(self.J), action] = 1\n",
    "        reward = self.get_reward(one_hot_action_seq)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return next_state, reward, terminal_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data(n, N, J):\n",
    "    \n",
    "    '''\n",
    "    Generates data for supervised learning\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    y= np.empty((n,J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.random.choice(N, size = J, replace = False)\n",
    "        sorted_args = np.argsort(X[i])\n",
    "        y[i] = np.argsort(sorted_args)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_rollout(T, env, agent):\n",
    "    \n",
    "    '''\n",
    "    Simulate one episode of length T at most\n",
    "    '''\n",
    "    \n",
    "    state_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    baseline_seq = []\n",
    "    \n",
    "    \n",
    "    state, terminal_state = env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while not terminal_state and t < T:\n",
    "        t +=1\n",
    "        \n",
    "        state_seq.append(state)\n",
    "        action, baseline = agent.take_action(state)\n",
    "        \n",
    "        state, reward, terminal_state = env.step(state, action)\n",
    "        \n",
    "        \n",
    "        action_seq.append(action)\n",
    "        reward_seq.append(reward)\n",
    "        baseline_seq.append(baseline)\n",
    "        \n",
    "    return np.array(state_seq), np.array(action_seq), np.array(reward_seq), np.squeeze(np.array(baseline_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "J = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n = batch_size * 10\n",
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(500, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77c0e5fb61147aab97f6cf645b957c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch: 1', max=10, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss = 1.663017\n",
      "reward = 0.647812\n",
      "\n",
      "val loss = 1.608635\n",
      "val reward = 0.665200\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNetLever(sess, N, J, lr = 1e-3, embedding_size= 128, training_mode = 'supervised')\n",
    "    env = LeverEnv(N, J)\n",
    "    commNet.supervised_train(X, y, val_X, val_y, env, batch_size = batch_size, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d018163f9eab47c181d112b2fb0930de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='REINFORCE', max=10, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNetLever(sess, N, J, lr = 1e-3, embedding_size= 128, training_mode = 'reinforce')\n",
    "    env = LeverEnv(N, J)\n",
    "    \n",
    "    history = commNet.reinforce_train(env, n_episodes = 10, T =64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEWCAYAAACKSkfIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFLZJREFUeJzt3X+QXWd93/H3R1qLX+aHkAUDspAsMAToODbaejZxIJkQN07TyhSnxcHQKDMq00lUA2lSnE7apG5awgRS2omnwXbcmKCpPXGAiiaDwYQo0ylKtAuuwXbdKNsICZuyyDLgQCwv++0fexRdrdfeK2l3z3qf92vmju55znPP+Z4zdz8697n3PjdVhSSpDWv6LkCStHwMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6Uk+S/E6SX+27DrXF0NeqlOQvk3wnyaNJvtoF7Ll91yX1zdDXavb3q+pc4GLgEuAX+ygiyUgf+5XmY+hr1auqrwJ3Mhv+JHlGkvcn+XKS/5fkt5I8q1u3L8lV3f0fSFJJ/m63/CNJ7u7uvzzJHyU5muTrSfYkecGJfXavNN6T5B7gr5KMJLkkyeeTfCvJ7cAzl/dMSIa+GpDkfODHgINd0/uAVzL7n8ArgE3Av+7W7QN+qLv/BmAS+MGB5X0nNgu8F3gp8GpgM/Arc3b9k8CPAy9g9m/t48DvAi8Efg+46uyPTjo9hr5Ws48n+RZwGPga8MtJAvwT4N1V9XBVfQv498DV3WP2cWrIv3dg+Qe79VTVwar6dFU9VlVTwG8M9DvhP1XV4ar6DjAGnAN8sKoer6o7gANLcMzSUzL0tZq9qaqey+yV+/cA5wEbgWcDE0keSfII8MmuHeBzwCuTvJjZVwIfBjYnOQ+4FPgTgCQvSnJbkq8k+SbwkW77gw4P3H8p8JU6dYbDQ4t3qNJwDH2telW1D/gd4P3A14HvAK+tqhd0t+d3b/hSVd8GJoB3Al+qquPA/wR+DviLqvp6t9n3AgVcVFXPA97G7JDPKbseuP8QsKl7pXHCyxbxMKWhGPpqxQeBy4GLgJuA/5DkRQBJNiX50YG++4DdnBy//+M5ywDPBR4FHkmyCfiFBfb/OWAauLZ7U/fNzL5ykJaVoa8mdOPuHwb+FfAeZt/U3d8NzdwFvGqg+z5mQ/1PnmQZ4N8ArwO+AfwB8NEF9n8ceDOwEzgGvGWhx0hLIf6IiiS1wyt9SWqIoS9JDTH0Jakhhr4kNWTFTQR13nnn1datW/suQ5KeViYmJr5eVRsX6rfiQn/r1q2Mj4/3XYYkPa0kGeob3g7vSFJDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+qvUxKFj3PDZg0wcOtZ3KZJWkBX3OX2dvYlDx7jm5v0cn55h3cga9uwaY/uW9X2XJWkF8Ep/Fdo/eZTj0zPMFDw+PcP+yaN9lyRphTD0V6GxbRtYN7KGtYFzRtYwtm1D3yVJWiEc3lmFtm9Zz55dY+yfPMrYtg0O7Uj6G4b+KrV9y3rDXtITOLwjSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH01wQnopFl+OUurnhPQSSd5pa9VzwnopJMMfa16TkAnneTwjlY9J6CTTjL01QQnoJNmObwjSQ0x9CWpIYa+JDXE0JekhgwV+kmuSPJAkoNJrptn/c4kU0nu7m67Bta9LMmnktyf5L4kWxevfEnS6Vjw0ztJ1gI3AJcDR4ADSfZW1X1zut5eVbvn2cSHgX9XVZ9Oci4wc7ZFS5LOzDBX+pcCB6tqsqqOA7cBVw6z8SSvAUaq6tMAVfVoVX37jKuVJJ2VYUJ/E3B4YPlI1zbXVUnuSXJHks1d2yuBR5J8NMkXkvx698rhFEnekWQ8yfjU1NRpH4QkaTjDhH7maas5y58AtlbVRcBdwK1d+wjweuDngb8NbAN2PmFjVTdW1WhVjW7cuHHI0iWdKWcdPam1czHMN3KPAJsHls8HHhzsUFWDM1jdBLxv4LFfqKpJgCQfB8aA3z7TgiWdHWcdPanFczHMlf4B4MIkFyRZB1wN7B3skOQlA4s7gPsHHrs+yYnL9x8G5r4BLGkZOevoSS2eiwWv9KtqOslu4E5gLXBLVd2b5HpgvKr2Atcm2QFMAw/TDeFU1XeT/DzwmSQBJph9JSCpJydmHX18eqb5WUdbPBepmjs836/R0dEaHx/vuwxpVZs4dMxZRzur5Vwkmaiq0YX6Ocum1CBnHT2ptXPhNAyS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0JeWUWuTe2l4y/Xc8MtZ0jJpcXIvDWc5nxte6UvLpMXJvTSc5XxuGPrSMjkxudfa0MzkXhrOcj43nHBNWkarZXIvLb6zfW444Zq0ArU2uZeGt1zPDYd3JKkhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYMFfpJrkjyQJKDSa6bZ/3OJFNJ7u5uuwbWfXegfe9iFi9JOj0L/jB6krXADcDlwBHgQJK9VXXfnK63V9XueTbxnaq6+OxLlSSdrWGu9C8FDlbVZFUdB24DrlzasiRJS2GY0N8EHB5YPtK1zXVVknuS3JFk80D7M5OMJ9mf5E3z7SDJO7o+41NTU8NXL0k6LcOEfuZpqznLnwC2VtVFwF3ArQPrXlZVo8BbgQ8mefkTNlZ1Y1WNVtXoxo0bhyxdknS6hgn9I8Dglfv5wIODHarqaFU91i3eBGwfWPdg9+8k8MfAJWdRryTpLAwT+geAC5NckGQdcDVwyqdwkrxkYHEHcH/Xvj7JM7r75wGXAXPfAJYkLZMFP71TVdNJdgN3AmuBW6rq3iTXA+NVtRe4NskOYBp4GNjZPfzVwIeSzDD7H8yvzfOpH0nSMknV3OH5fo2Ojtb4+HjfZUjS00qSie7906fkN3IlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDVkVYX+xKFj3PDZg0wcOtZ3KZK0Ii34w+hPFxOHjnHNzfs5Pj3DupE17Nk1xvYt6/suS5JWlFVzpb9/8ijHp2eYKXh8eob9k0f7LkmSVpxVE/pj2zawbmQNawPnjKxhbNuGvkuSpBVn1QzvbN+ynj27xtg/eZSxbRsc2pGkeaya0IfZ4DfsJenJrZrhHUnSwgx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkOGCv0kVyR5IMnBJNfNs35nkqkkd3e3XXPWPy/JV5L85mIVLkk6fQtOw5BkLXADcDlwBDiQZG9V3Ten6+1VtftJNvNvgX1nVakk6awNc6V/KXCwqiar6jhwG3DlsDtIsh14MfCpMytRkrRYhgn9TcDhgeUjXdtcVyW5J8kdSTYDJFkDfAD4hafaQZJ3JBlPMj41NTVk6ZKk0zVM6Geetpqz/Alga1VdBNwF3Nq1/wzwh1V1mKdQVTdW1WhVjW7cuHGIkiRJZ2KYqZWPAJsHls8HHhzsUFWDP1N1E/C+7v73Aa9P8jPAucC6JI9W1RPeDJYkLb1hQv8AcGGSC4CvAFcDbx3skOQlVfVQt7gDuB+gqq4Z6LMTGDXwJak/C4Z+VU0n2Q3cCawFbqmqe5NcD4xX1V7g2iQ7gGngYWDnEtYsSTpDqZo7PN+v0dHRGh8f77sMSXpaSTJRVaML9fMbuZLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0JfVi4tAxbvjsQSYOHeu7lKaM9F2ApPZMHDrGNTfv5/j0DOtG1rBn1xjbt6zvu6wmeKUvadntnzzK8ekZZgoen55h/+TRvktqhqEvadmNbdvAupE1rA2cM7KGsW0b+i6pGUOFfpIrkjyQ5GCS6+ZZvzPJVJK7u9uurn1Lkomu7d4k/3SxD0DS08/2LevZs2uMn/s7r3JoZ5ktOKafZC1wA3A5cAQ4kGRvVd03p+vtVbV7TttDwPdX1WNJzgW+1D32wcUoXtLT1/Yt6w37HgxzpX8pcLCqJqvqOHAbcOUwG6+q41X1WLf4jCH3J0laIsOE8Cbg8MDyka5trquS3JPkjiSbTzQm2Zzknm4b75vvKj/JO5KMJxmfmpo6zUOQJA1rmNDPPG01Z/kTwNaqugi4C7j1bzpWHe7aXwH8VJIXP2FjVTdW1WhVjW7cuHH46iVJp2WY0D8CbB5YPh845Wq9qo4ODOPcBGyfu5HuCv9e4PVnVqok6WwNE/oHgAuTXJBkHXA1sHewQ5KXDCzuAO7v2s9P8qzu/nrgMuCBxShcknT6Fvz0TlVNJ9kN3AmsBW6pqnuTXA+MV9Ve4NokO4Bp4GFgZ/fwVwMfSFLMDhO9v6q+uATHIUkaQqrmDs/3a3R0tMbHx/suQ5KeVpJMVNXoQv38CKWWlJNqSSuLE65pyTiplrTyeKWvJeOkWtLKY+hryTiplrTyOLyjJXNiUq39k0cZ27bBoR1pBTD0taScVEtaWRzekaSGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaMlToJ7kiyQNJDia5bp71O5NMJbm7u+3q2i9O8rkk9ya5J8lbFvsAJEnDG1moQ5K1wA3A5cAR4ECSvVV135yut1fV7jlt3wb+cVX9eZKXAhNJ7qyqRxajeEnS6RnmSv9S4GBVTVbVceA24MphNl5V/6eq/ry7/yDwNWDjmRYrSTo7w4T+JuDwwPKRrm2uq7ohnDuSbJ67MsmlwDrgL86oUknSWRsm9DNPW81Z/gSwtaouAu4Cbj1lA8lLgN8FfrqqZp6wg+QdScaTjE9NTQ1XuSTptA0T+keAwSv384EHBztU1dGqeqxbvAnYfmJdkucBfwD8UlXtn28HVXVjVY1W1ejGjY7+SNJSGSb0DwAXJrkgyTrgamDvYIfuSv6EHcD9Xfs64GPAh6vq9xanZEnSmVrw0ztVNZ1kN3AnsBa4paruTXI9MF5Ve4Frk+wApoGHgZ3dw/8R8AZgQ5ITbTur6u7FPYyVY+LQMfZPHmVs2wa2b1nfdzmSdIpUzR2e79fo6GiNj4/3XcYZmTh0jGtu3s/x6RnWjaxhz64xg1/SskgyUVWjC/XzG7mLaP/kUY5PzzBT8Pj0DPsnj/ZdkiSdwtBfRGPbNrBuZA1rA+eMrGFs24a+S5KkUyw4pq/hbd+ynj27xhzTl7RiGfqLbPuW9Ya9pBXL4R1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUkBU3DUOSKeDQWWziPODri1TO053n4lSej1N5Pk5aDediS1UtOE3xigv9s5VkfJj5J1rguTiV5+NUno+TWjoXDu9IUkMMfUlqyGoM/Rv7LmAF8VycyvNxKs/HSc2ci1U3pi9JenKr8UpfkvQkDH1JasiqCf0kVyR5IMnBJNf1XU+fkmxO8tkk9ye5N8k7+66pb0nWJvlCkv/edy19S/KCJHck+d/dc+T7+q6pT0ne3f2dfCnJf03yzL5rWkqrIvSTrAVuAH4MeA3wk0le029VvZoG/nlVvRoYA3628fMB8E7g/r6LWCH+I/DJqvoe4Htp+Lwk2QRcC4xW1d8C1gJX91vV0loVoQ9cChysqsmqOg7cBlzZc029qaqHqurz3f1vMftHvanfqvqT5Hzgx4Gb+66lb0meB7wB+G2AqjpeVY/0W1XvRoBnJRkBng082HM9S2q1hP4m4PDA8hEaDrlBSbYClwB/2m8lvfog8C+Amb4LWQG2AVPAf+mGu25O8py+i+pLVX0FeD/wZeAh4BtV9al+q1paqyX0M09b859FTXIu8PvAu6rqm33X04ckfw/4WlVN9F3LCjECvA74z1V1CfBXQLPvgSVZz+yowAXAS4HnJHlbv1UtrdUS+keAzQPL57PKX6ItJMk5zAb+nqr6aN/19OgyYEeSv2R22O+Hk3yk35J6dQQ4UlUnXvndwex/Aq36EeD/VtVUVT0OfBT4/p5rWlKrJfQPABcmuSDJOmbfiNnbc029SRJmx2zvr6rf6LuePlXVL1bV+VW1ldnnxR9V1aq+knsqVfVV4HCSV3VNbwTu67Gkvn0ZGEvy7O7v5o2s8je2R/ouYDFU1XSS3cCdzL77fktV3dtzWX26DHg78MUkd3dt/7Kq/rDHmrRy/DNgT3eBNAn8dM/19Kaq/jTJHcDnmf3U2xdY5VMyOA2DJDVktQzvSJKGYOhLUkMMfUlqiKEvSQ0x9CWpIYa+tAiS/JAzeOrpwNCXpIYY+mpKkrcl+bMkdyf5UDfP/qNJPpDk80k+k2Rj1/fiJPuT3JPkY908LSR5RZK7kvyv7jEv7zZ/7sA89Xu6b3iS5NeS3Ndt5/09HboEGPpqSJJXA28BLquqi4HvAtcAzwE+X1WvA/YBv9w95MPAe6rqIuCLA+17gBuq6nuZnafloa79EuBdzP6mwzbgsiQvBP4B8NpuO7+6tEcpPTVDXy15I7AdONBNT/FGZsN5Bri96/MR4AeSPB94QVXt69pvBd6Q5LnApqr6GEBV/XVVfbvr82dVdaSqZoC7ga3AN4G/Bm5O8mbgRF+pF4a+WhLg1qq6uLu9qqp+ZZ5+TzU3yXzTeJ/w2MD97wIjVTXN7I/8/D7wJuCTp1mztKgMfbXkM8BPJHkRQJIXJtnC7N/BT3R93gr8j6r6BnAsyeu79rcD+7rfJTiS5E3dNp6R5NlPtsPuNw2e30129y7g4qU4MGlYq2KWTWkYVXVfkl8CPpVkDfA48LPM/pDIa5NMAN9gdtwf4KeA3+pCfXA2yrcDH0pyfbeNf/gUu30u8N+6H9sO8O5FPizptDjLppqX5NGqOrfvOqTl4PCOJDXEK31JaohX+pLUEENfkhpi6EtSQwx9SWqIoS9JDfn/eWVXyjKrfYoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x269f1042908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFb9JREFUeJzt3X+QXWd93/H3R16JBGSwIi8JCCEhAgTDECu7oZtoSFJDm9TJQJpCgBqNm8YodExiMc4PMKXpj0mbScAUEk+KYrtDhm0IQYIyCSmooHrGf6xgVxE28kLwqF4wqLCoAom0QVr22z/ucb0y2t0rea/u7rnv18yO733OOfd+93j3o2ef+5zzpKqQJK196/pdgCRpZRjoktQSBroktYSBLkktYaBLUksY6JLUEga6JLWEga5WSvJQkpf1uw7pcjLQJaklDHQNlCSvT/Jgkv+d5CNJnt60J8k7k3wtyTeT3Jfkhc2265M8kORMki8n+fX+fhfShRnoGhhJrgP+A/CLwNOAGeD9zeZ/CPwE8FzgKuDVwMlm213Ar1TVlcALgU9exrKlrg31uwDpMroBuLuqjgAkeQtwKsl24BxwJfBDwKeqanrBceeAa5J8pqpOAacua9VSl+yha5A8nU6vHICq+hadXviWqvok8IfAHcBXk+xL8uRm138CXA/MJLknyY9d5rqlrhjoGiRfAbY98iTJk4DNwJcBqurdVTUCvIDO0MtvNO2frqpXAE8FPgx84DLXLXXFQFebrU/yPY980QniX0pybZInAP8eOFxVDyX50SR/L8l64G+BvwO+k2RDkhuSPKWqzgGnge/07TuSlmCgq80+CvzfBV8vAd4G7AdOAM8GXtPs+2Tgj+mMj8/QGYp5e7NtN/BQktPAG4DXXab6pYsSF7iQpHawhy5JLWGgS1JLGOiS1BIGuiS1xGW9UvTqq6+u7du3X863lKQ1b2pq6utVNbzcfpc10Ldv387k5OTlfEtJWvOSzCy/l0MuktQaBroktYSBLkktYaBLUkssG+hJtiY5lGQ6ybEktzTt1yaZSHI0yWSSF/e+XEnSYrqZ5TIH3FpVR5JcCUwlOQj8HvBvquqvklzfPP+p3pUqSVrKsoFeVSfo3JmOqjqTZBrYAhSdO9QBPIXOvaYlSY8xNXOKieMnGduxmZFtm3r2Phc1D71ZqmsncBjYC3wsydvpDN38+CLH7AH2ADzzmc98HKVK0tozNXOKG+6c4OzcPBuG1jF+01jPQr3rD0WTbKRzH+m9VXUa+BfAm6pqK/AmOgvpfpeq2ldVo1U1Ojy87IVOktQqE8dPcnZunvmCc3PzTBw/ufxBl6irQG9WcdkPjFfVgab5RuCRx38O+KGoJD3G2I7NbBhaxxWB9UPrGNuxuWfvteyQS5LQ6X1PV9XtCzZ9BfhJ4H8A1wFf6EWBkrSWjWzbxPhNY6tmDH0XnSW47k9ytGm7DXg98K4kQ3TWX9zTmxIlaW0b2bapp0H+iG5mudwLZJHNIytbjiTpUnmlqCS1hIEuSS1hoEtSSxjoktQSBroktYSBLkktYaBLUksY6JLUEga6JLWEgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSS3SzYtFW4E+AHwDmgX1V9a4kfwY8r9ntKuAbVXVtzyqVJC2pmxWL5oBbq+pIkiuBqSQHq+rVj+yQ5B3AN3tVpCRped2sWHQCONE8PpNkGtgCPAD/f83RX6SzrqgkqU8uagw9yXZgJ3B4QfNLgK9W1QUXiU6yJ8lkksnZ2dlLrVOStIyuAz3JRmA/sLeqTi/Y9FrgTxc7rqr2VdVoVY0ODw9feqWSpCV1M4ZOkvV0wny8qg4saB8CfgEXi5akvlu2h96Mkd8FTFfV7Y/Z/DLgc1X1cC+KkyR1r5shl13AbuC6JEebr+ubba9hieEWSdLl080sl3uBLLLtn610QZKkS+OVopLUEga6JLWEgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktYSBLkktYaBLUksY6JLUEga6JLWEgS5JLdHNikVbkxxKMp3kWJJbFmz71SSfb9p/r7elSpKW0s2aonPArVV1JMmVwFSSg8D3A68AXlRV307y1F4WKklaWjcrFp0ATjSPzySZBrYArwd+t6q+3Wz7Wi8LlSQt7aLG0JNsB3YCh4HnAi9JcjjJPUl+dJFj9iSZTDI5Ozv7eOuVJC2i60BPshHYD+ytqtN0evebgDHgN4APJPmutUeral9VjVbV6PDw8AqVLUl6rK4CPcl6OmE+XlUHmuaHgQPV8SlgHri6N2VKkpbTzSyXAHcB01V1+4JNHwaua/Z5LrAB+HovipQkLa+bWS67gN3A/UmONm23AXcDdyf5LHAWuLGqqjdlSpKW080sl3uB7xobb7xuZcuRJF0qrxSVpJYw0CWpJQx0SWoJA12SWsJAl6SWMNClFTA1c4o7Dj3I1MypfpeiAdbNPHRJS5iaOcUNd05wdm6eDUPrGL9pjJFtm/pdlgaQPfQ1yN7go1bDuZg4fpKzc/PMF5ybm2fi+Mm+1aLBZg99jbE3+KjVci7Gdmxmw9A6zs3Ns35oHWM7Nl/2GiQw0NecC/UGBzXQV8u5GNm2ifGbxpg4fpKxHZsH9v+H+s9AX2PsDT5qNZ2LkW2bDHL1XS7n/bRGR0drcnLysr1fW03NnLI32PBcaBAkmaqq0eX2s4e+BtkbfJTnQnqUs1wkqSUMdElqiW5WLNqa5FCS6STHktzStP/rJF9OcrT5ur735UqSFtPNGPoccGtVHUlyJTCV5GCz7Z1V9fbelSdJ6lY3KxadAE40j88kmQa29LowSdLFuagx9CTbgZ3A4abpjUnuS3J3EqcaSFIfdR3oSTYC+4G9VXUa+CPg2cC1dHrw71jkuD1JJpNMzs7OrkDJkqQL6SrQk6ynE+bjVXUAoKq+WlXfqap54I+BF1/o2KraV1WjVTU6PDy8UnVLkh6jm1kuAe4Cpqvq9gXtT1uw2z8GPrvy5UmSutXNLJddwG7g/iRHm7bbgNcmuRYo4CHgV3pSoSSpK93McrkXyAU2fXTly5EkXSqvFJWkljDQJaklDHRJagkDXZJawkCXpJYw0CWpJQx0SWoJA12SWsJAl6SWMNAlqSUMdElqCQNdklrCQJekljDQJaklDHRJagkDXZJaopsl6LYmOZRkOsmxJLc8ZvuvJ6kkV/euTEnScrpZgm4OuLWqjiS5EphKcrCqHkiyFfgHwBd7WqUkaVnL9tCr6kRVHWkenwGmgS3N5ncCv0lnXVFJUh9d1Bh6ku3ATuBwkpcDX66qzyxzzJ4kk0kmZ2dnL7lQSdLSug70JBuB/cBeOsMwbwX+1XLHVdW+qhqtqtHh4eFLLlSStLSuAj3JejphPl5VB4BnA88CPpPkIeAZwJEkP9CrQiVJS1v2Q9EkAe4CpqvqdoCquh946oJ9HgJGq+rrPapTkrSMbnrou4DdwHVJjjZf1/e4Lklr2NTMKe449CBTM6f6XcpAWbaHXlX3Allmn+0rVZCktW1q5hQ33DnB2bl5NgytY/ymMUa2bep3WQPBK0UlraiJ4yc5OzfPfMG5uXkmjp/sd0kDw0CXtKLGdmxmw9A6rgisH1rH2I7N/S5pYHRzpagkdW1k2ybGbxpj4vhJxnZsdrjlMjLQJa24kW2bDPI+cMhFklrCQJekljDQJbXWoM2HdwxdUisN4nx4e+iSWmkQ58Mb6JJaaRDnwzvkIqmVBnE+vIEuqbUGbT68Qy6S1BIGuiS1hIEuSS2xbKAn2ZrkUJLpJMeS3NK0/7sk9zULXnw8ydN7X64kaTHd9NDngFur6vnAGHBzkmuA36+qF1XVtcBf0MWC0ZJ6a9CujNT5ulmx6ARwonl8Jsk0sKWqHliw25OA6k2JkroxiFdG6nwXNYaeZDuwEzjcPP+dJF8CbmCRHnqSPUkmk0zOzs4+vmolLWoQr4zU+boO9CQbgf3A3qo6DVBVb62qrcA48MYLHVdV+6pqtKpGh4eHV6JmSRcwiFdG6nxdXViUZD2dMB+vqgMX2OW/AH8J/PYK1ibpIgzilZE637KBniTAXcB0Vd2+oP05VfWF5unLgc/1pkRJ3Rq0KyN1vm566LuA3cD9SY42bbcBv5zkecA8MAO8oTclSpK60c0sl3uBXGDTR1e+HEnSpfJKUUlqCQNdklrCQJekljDQJaklDHRJagkDXZJawkCXpJYw0CWpJQx0SWoJA12SWsJAl6SWMNAlqSUMdElqCQNdklrCQJekllg20JNsTXIoyXSSY0luadp/P8nnktyX5ENJrup9uZKkxXTTQ58Dbq2q5wNjwM1JrgEOAi+sqhcBfwO8pXdlSpKWs2ygV9WJqjrSPD4DTANbqurjVTXX7DYBPKN3ZUqSlnNRY+hJtgM7gcOP2fTPgb9a5Jg9SSaTTM7Ozl5KjZKkLnQd6Ek2AvuBvVV1ekH7W+kMy4xf6Liq2ldVo1U1Ojw8/HjrlSQtYtlFogGSrKcT5uNVdWBB+43AzwEvrarqTYmSpG4sG+hJAtwFTFfV7Qvafwb4LeAnq+r/9K5ESVI3uumh7wJ2A/cnOdq03Qa8G3gCcLCT+UxU1Rt6UqUkaVnLBnpV3QvkAps+uvLlSJIulVeKSlJLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktYSBLkktYaBLUksY6JLUEmsi0KdmTnHHoQeZmjnV71IkadXq6va5/TQ1c4ob7pzg7Nw8G4bWMX7TGCPbNvW7LEladVZ9D33i+EnOzs0zX3Bubp6J4yf7XZIkrUqrPtDHdmxmw9A6rgisH1rH2I7N/S5JklalVT/kMrJtE+M3jTFx/CRjOzY73CJJi1i2h55ka5JDSaaTHEtyS9P+qub5fJLRXhY5sm0TN//9HzTMJWkJ3fTQ54Bbq+pIkiuBqSQHgc8CvwC8p5cFSpK6082KRSeAE83jM0mmgS1VdRCgWX5uIEzNnHLoR9KqdVFj6Em2AzuBw70oZjVz+qSk1a7rWS5JNgL7gb1VdfoijtuTZDLJ5Ozs7KXUuCo4fVLSatdVoCdZTyfMx6vqwMW8QVXtq6rRqhodHh6+lBpXBadPSlrtlh1ySWeQ/C5guqpu731Jq5PTJyWtdt2Moe8CdgP3JznatN0GPAH4A2AY+MskR6vqp3tT5uowsm2TQS5p1epmlsu9wGJTWT60suVIki7Vqr/0X5LUHQNdklrCQJekljDQJaklDHRJagkDXZJawkCXpJYw0CWpJQx0SWoJA12SWsJAl6SWMNAlqSUMdElqCQNdklrCQJekljDQJakllg30JFuTHEoyneRYklua9u9LcjDJF5r/upSPJPVRNz30OeDWqno+MAbcnOQa4M3AJ6rqOcAnmueSpD5ZNtCr6kRVHWkenwGmgS3AK4D3Nru9F/j5XhUpSVreRY2hJ9kO7AQOA99fVSegE/rAUxc5Zk+SySSTs7Ozj69aSdKiug70JBuB/cDeqjrd7XFVta+qRqtqdHh4+FJqlCR1oatAT7KeTpiPV9WBpvmrSZ7WbH8a8LXelChJ6kY3s1wC3AVMV9XtCzZ9BLixeXwj8F9XvjytZlMzp7jj0INMzZzqdymSgKEu9tkF7AbuT3K0absN+F3gA0l+Gfgi8KrelKjVaGrmFDfcOcHZuXk2DK1j/KYxRrY5c1Xqp2UDvaruBbLI5peubDlaKyaOn+Ts3DzzBefm5pk4ftJAl/rMK0V1ScZ2bGbD0DquCKwfWsfYjs39LkkaeN0MuUjfZWTbJsZvGmPi+EnGdmy2dy6tAga6LtnItk0GubSKOOQiSS1hoEtSSxjoktQSBroktYSBLkktYaBLUkukqi7fmyWzwMwlHn418PUVLGet83w8ynNxPs/H+dpwPrZV1bK3q72sgf54JJmsqtF+17FaeD4e5bk4n+fjfIN0PhxykaSWMNAlqSXWUqDv63cBq4zn41Gei/N5Ps43MOdjzYyhS5KWtpZ66JKkJRjoktQSayLQk/xMks8neTDJm/tdT78k2ZrkUJLpJMeS3NLvmlaDJFck+eskf9HvWvotyVVJPpjkc83PyY/1u6Z+SfKm5vfks0n+NMn39LumXlv1gZ7kCuAO4B8B1wCvTXJNf6vqmzng1qp6PjAG3DzA52KhW4DpfhexSrwL+G9V9UPADzOg5yXJFuDXgNGqeiFwBfCa/lbVe6s+0IEXAw9W1fGqOgu8H3hFn2vqi6o6UVVHmsdn6PyybulvVf2V5BnAzwJ39ruWfkvyZOAngLsAqupsVX2jv1X11RDwvUmGgCcCX+lzPT23FgJ9C/ClBc8fZsBDDCDJdmAncLi/lfTdfwR+E5jvdyGrwA5gFvjPzRDUnUme1O+i+qGqvgy8HfgicAL4ZlV9vL9V9d5aCPRcoG2g51om2QjsB/ZW1el+19MvSX4O+FpVTfW7llViCPgR4I+qaifwt8BAfuaUZBOdv+SfBTwdeFKS1/W3qt5bC4H+MLB1wfNnMAB/Oi0myXo6YT5eVQf6XU+f7QJenuQhOkNx1yV5X39L6quHgYer6pG/2j5IJ+AH0cuA/1lVs1V1DjgA/Hifa+q5tRDonwaek+RZSTbQ+WDjI32uqS+ShM746HRV3d7vevqtqt5SVc+oqu10fi4+WVWt74Utpqr+F/ClJM9rml4KPNDHkvrpi8BYkic2vzcvZQA+IB7qdwHLqaq5JG8EPkbnk+q7q+pYn8vql13AbuD+JEebttuq6qN9rEmry68C403n5zjwS32upy+q6nCSDwJH6MwO+2sG4BYAXvovSS2xFoZcJEldMNAlqSUMdElqCQNdklrCQJekljDQpcdI8lPeuVFrkYEuSS1hoGvNSvK6JJ9KcjTJe5r7on8ryTuSHEnyiSTDzb7XJplIcl+SDzX3+iDJDyb570k+0xzz7OblNy64r/h4c7UhSUaS3JNkKsnHkjytaf+1JA80r//+vpwQDTwDXWtSkucDrwZ2VdW1wHeAG4AnAUeq6keAe4Dfbg75E+C3qupFwP0L2seBO6rqh+nc6+NE074T2EvnHvw7gF3NfXT+AHhlVY0AdwO/0+z/ZmBn8/pv6M13LS1t1V/6Ly3ipcAI8Omm8/y9wNfo3Eb3z5p93gccSPIU4Kqquqdpfy/w50muBLZU1YcAqurvAJrX+1RVPdw8PwpsB74BvBA42OxzBY/+A3AfnUvuPwx8uDffsrQ0A11rVYD3VtVbzmtM3vaY/Za6t8WFbs38iG8vePwdOr8rAY5V1YWWdftZOotLvBx4W5IXVNXcEq8vrTiHXLRWfQJ4ZZKnAiT5viTb6PxMv7LZ558C91bVN4FTSV7StO8G7mnuJf9wkp9vXuMJSZ64xHt+Hhh+ZJ3OJOuTvCDJOmBrVR2is9jGVcDGFf1upS7YQ9eaVFUPJPmXwMebQD0H3ExnUYcXJJkCvklnnB3gRuA/NYG98C6Eu4H3JPm3zWu8aon3PJvklcC7m2GcITorJv0N8L6mLcA7B3zpN/WJd1tUqyT5VlXZO9ZAcshFklrCHroktYQ9dElqCQNdklrCQJekljDQJaklDHRJaon/B5jKhA4BoRTZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x269f1084080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Reward')\n",
    "plt.plot(history['reward'], '.')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "plt.title('Loss')\n",
    "plt.plot(history['loss'], '.')\n",
    "plt.xlabel('epoches')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
