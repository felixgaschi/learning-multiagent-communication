{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNet:\n",
    "    \n",
    "    def __init__(self, sess, N, J, embedding_size = 128, lr = 1e-3, training_mode = 'supervised', gamma = 1):\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        \n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        if training_mode == 'supervised':\n",
    "            self.build_supervised()\n",
    "            with tf.variable_scope('Supervised_optimizer'):\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "                \n",
    "        elif training_mode == 'reinforce':\n",
    "            self.gamma = gamma\n",
    "            self.build_reinforce()\n",
    "            with tf.variable_scope('Reinforce_optimizer'):\n",
    "                self.train_op =  tf.train.AdamOptimizer(lr).minimize(self.reinforce_loss)\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError(\"Unknown training mode: %s\" % training_mode))\n",
    "        \n",
    "        print(\"All variables\")\n",
    "        for var in tf.global_variables():\n",
    "            print(var)\n",
    "            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            embedded_identities = tf.nn.embedding_lookup(identity_embeddings, inputs)\n",
    "            \n",
    "        return tf.unstack(embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None:\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            else:\n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "            \n",
    "            W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "            \n",
    "            dense1 =  tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1))\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2))\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W)\n",
    "        \n",
    "            return policy_logit\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_actions(self, policy_logit):\n",
    "        \n",
    "        \n",
    "        action = tf.multinomial(policy_logit, num_samples = 1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in h2_seq]\n",
    "        \n",
    "        self.action_seq = [self.sample_actions(policy_logit) for policy_logit in self.policy_logit_seq]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_supervised(self):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        self.one_hot_action_seq = [tf.one_hot(tf.reshape(action, [-1]), depth = self.J) for action in self.action_seq]\n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_sum(tf.add_n(supervised_loss_seq))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def supervised_train(self, X, y, val_X, val_y, env, batch_size = 32, epochs = 1):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            np.random.shuffle(data_inds)\n",
    "            supervised_loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                _, supervised_loss, one_hot_action_seq = sess.run([self.train_op, self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "                supervised_loss_sum += supervised_loss\n",
    "                reward_sum += env.get_reward(one_hot_action_seq)\n",
    "            \n",
    "            print(\"loss = %f\" % (supervised_loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            val_supervised_loss, val_one_hot_action_seq = sess.run([self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss / val_n))\n",
    "            print('val reward = %f' % (env.get_reward(val_one_hot_action_seq) / val_n))\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        self.advantage = tf.placeholder(tf.float32, shape = (None, self.J))\n",
    "        unstacked_advantage = tf.unstack(self.advantage, axis = 1)\n",
    "        \n",
    "        self.action_taken = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_action_taken = tf.unstack(self.action_taken, axis = 1)\n",
    "        \n",
    "        self.neg_log_p_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_action_taken[j],\n",
    "                                                    logits=self.policy_logit_seq[j]) for j in range(self.J)]\n",
    "        #surrogate loss\n",
    "        self.reinforce_loss =  tf.add_n([tf.reduce_mean(tf.multiply(unstacked_advantage[j], self.neg_log_p_seq[j]), axis = 0) for j in range(self.J)])\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        action_seq= self.sess.run(self.action_seq, {self.inputs: [state]})\n",
    "        \n",
    "        return [x[0,0] for x in action_seq]\n",
    "    \n",
    "    def reinforce_train(self, env, n_episodes, T):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        baseline = np.zeros(self.N)\n",
    "        \n",
    "        discount_factors = np.array([self.gamma ** t for t in range(T)])\n",
    "        \n",
    "        history = {'reward' : [],  'loss': []}\n",
    "    \n",
    "        \n",
    "        for _ in tqdm(range(n_episodes), \"REINFORCE\"):\n",
    "            \n",
    "            # todo: change code to avoid this seq_seq name (sequence of sequence)\n",
    "            state_seq, action_seq, reward_seq = policy_rollout(T, env, commNet)\n",
    "            t = reward_seq.shape[0]\n",
    "            G = np.sum(reward_seq * discount_factors[:t])\n",
    "            \n",
    "            history['reward'].append(np.mean(reward_seq))\n",
    "            \n",
    "            \n",
    "            \n",
    "            feed_dict = {}\n",
    "            feed_dict[self.inputs] = state_seq\n",
    "            feed_dict[self.advantage] = G * (1 - baseline[state_seq])\n",
    "            feed_dict[self.action_taken] = action_seq\n",
    "            \n",
    "            _, loss = self.sess.run([self.train_op, self.reinforce_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "            # update baseline\n",
    "            raveled_state_seq = state_seq.ravel()\n",
    "            repeated_reward_seq = np.repeat(reward_seq, self.J)\n",
    "            distinct_states = list(set(raveled_state_seq))\n",
    "            state_count = np.zeros(self.N)\n",
    "            reward_sum = np.zeros(self.N)\n",
    "            \n",
    "            for s,r in zip(raveled_state_seq, repeated_reward_seq):\n",
    "                state_count[s] += 1\n",
    "                reward_sum[s] += r\n",
    "            baseline[distinct_states] = 0.99 * baseline[distinct_states] + 0.01 * reward_sum[distinct_states]/state_count[distinct_states]\n",
    "        print(baseline)\n",
    "        return history\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeverEnv:\n",
    "    \n",
    "    def __init__(self, N, J):\n",
    "        \n",
    "        self.J = J\n",
    "        self.N = N\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        state = np.sort(np.random.choice(self.N, size = self.J, replace = False))\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return state, terminal_state        \n",
    "    \n",
    "    def get_reward(self, one_hot_action_seq):        \n",
    "        \n",
    "        reward = np.sum(np.sum(one_hot_action_seq, axis = 0) > 0) /self.J\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        \n",
    "        next_state = np.sort(np.random.choice(self.N, size = self.J, replace = False))\n",
    "        \n",
    "        one_hot_action_seq = np.zeros((self.J, self.J))\n",
    "        one_hot_action_seq[range(self.J), action] = 1\n",
    "        reward = self.get_reward(one_hot_action_seq)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return next_state, reward, terminal_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation for supervised learning\n",
    "def generate_data(n, N, J):\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.sort(np.random.choice(N, size = J, replace = False))\n",
    "        \n",
    "    y = np.tile([j for j in range(J)], (n,1))\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode generation for reinforcement learning\n",
    "def policy_rollout(T, env, agent):\n",
    "    \n",
    "    state_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    \n",
    "    \n",
    "    state, terminal_state = env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while not terminal_state and t < T:\n",
    "        t +=1\n",
    "        \n",
    "        state_seq.append(state)\n",
    "        action = agent.take_action(state)\n",
    "        \n",
    "        state, reward, terminal_state = env.step(state, action)\n",
    "        \n",
    "        \n",
    "        action_seq.append(action)\n",
    "        reward_seq.append(reward)\n",
    "        \n",
    "    return np.array(state_seq), np.array(action_seq), np.array(reward_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "J = 5\n",
    "batch_size = 32\n",
    "n = batch_size * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(1024, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables\n",
      "<tf.Variable 'Encoder/identity_embeddings:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/W:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/beta1_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/beta2_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/Adam:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/Adam_1:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/Adam:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/Adam_1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/Adam:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/Adam_1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/Adam:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/Adam_1:0' shape=(128, 5) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE: 100%|███████████████████████████████████████████████████████████████████| 3000/3000 [00:30<00:00, 97.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19799576 0.19688435 0.19706453 0.19737203 0.19723747 0.19759931\n",
      " 0.19697883 0.19723546 0.19709328 0.19709328 0.19791355 0.19764881\n",
      " 0.197004   0.19661998 0.19801766 0.19729502 0.19737453 0.1974748\n",
      " 0.19760287 0.19655171 0.19715084 0.1968496  0.19649066 0.19648133\n",
      " 0.1972648  0.19787206 0.19742327 0.19787118 0.19729216 0.1979155\n",
      " 0.1971526  0.19655278 0.19739724 0.1978499  0.19715902 0.19794125\n",
      " 0.19759906 0.19842629 0.19769295 0.19691261 0.19678564 0.19765982\n",
      " 0.19795507 0.19762633 0.1968181  0.1971802  0.19757406 0.19771913\n",
      " 0.19811324 0.19662033 0.19782818 0.19754955 0.19799597 0.1968181\n",
      " 0.19726338 0.19688937 0.1967709  0.19789378 0.19771816 0.19715084\n",
      " 0.19735375 0.19641249 0.19626377 0.19720754 0.1972452  0.19731784\n",
      " 0.1974493  0.19726311 0.19767061 0.19767257 0.19785124 0.19672275\n",
      " 0.19737258 0.19678596 0.19742622 0.19747454 0.19771768 0.19752764\n",
      " 0.19799722 0.19744982 0.19648352 0.19745387 0.19764635 0.19712206\n",
      " 0.19739913 0.19744904 0.19607041 0.19723546 0.19691584 0.19637972\n",
      " 0.19785102 0.19774027 0.19622643 0.19706392 0.19750083 0.19720754\n",
      " 0.19697374 0.19747559 0.19720754 0.1977624 ]\n",
      "[[12 13 82 87 91]\n",
      " [24 36 64 84 88]\n",
      " [13 41 63 71 76]]\n",
      "[[3 3 3 3 3]\n",
      " [3 3 3 3 3]\n",
      " [3 3 3 3 3]]\n",
      "[0.2 0.2 0.2]\n",
      "0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNet(sess, N, J, lr = 1e-2, training_mode = 'reinforce')\n",
    "    env = LeverEnv(N, J)\n",
    "    #commNet.supervised_train(X, y, val_X, val_y, env, batch_size = batch_size, epochs = 10)\n",
    "    \n",
    "    history = commNet.reinforce_train(env, n_episodes = 3000, T = 3)\n",
    "\n",
    "    \n",
    "    state_seq, action_seq, reward_seq = policy_rollout(3, env, commNet)\n",
    "    \n",
    "    print(state_seq)\n",
    "    print(action_seq)\n",
    "    print(reward_seq)\n",
    "    G = np.sum(reward_seq)\n",
    "    print(G)\n",
    "    \n",
    "    feed_dict = {}\n",
    "    feed_dict[commNet.inputs] = state_seq\n",
    "    feed_dict[commNet.advantage] = G * np.ones((3, J))\n",
    "    feed_dict[commNet.action_taken] = action_seq\n",
    "\n",
    "    commNet.sess.run(commNet.train_op, feed_dict = feed_dict)\n",
    "    \n",
    "    rv = sess.run([commNet.policy_logit_seq, commNet.neg_log_p_seq, commNet.reinforce_loss, commNet.policy_logit_seq], feed_dict= feed_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b9eaaeb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19b9ef3b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['reward'], '*')\n",
    "plt.show()\n",
    "plt.plot(history['loss'], '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ -13.03694  ,   52.67092  , -223.61464  ,  291.23468  ,\n",
       "         -245.1621   ],\n",
       "        [  -4.3815255,   18.725832 ,  -77.3276   ,  100.38756  ,\n",
       "          -85.11657  ],\n",
       "        [  -5.284323 ,   21.093325 ,  -88.27678  ,  114.6259   ,\n",
       "          -96.578445 ]], dtype=float32),\n",
       " array([[ -11.645896 ,   50.23421  , -208.8404   ,  270.70715  ,\n",
       "         -229.062    ],\n",
       "        [  -2.8375664,   15.350157 ,  -63.11995  ,   81.56537  ,\n",
       "          -69.68394  ],\n",
       "        [  -4.530597 ,   17.5512   ,  -72.333694 ,   94.89189  ,\n",
       "          -80.23583  ]], dtype=float32),\n",
       " array([[ -13.083209,   52.928978, -222.83713 ,  290.464   , -244.83418 ],\n",
       "        [  -3.922386,   14.801664,  -61.89308 ,   81.03401 ,  -68.29156 ],\n",
       "        [  -5.097569,   20.87211 ,  -89.012344,  115.81691 ,  -97.30127 ]],\n",
       "       dtype=float32),\n",
       " array([[  -9.3632555,   43.687996 , -179.46361  ,  232.24626  ,\n",
       "         -197.54274  ],\n",
       "        [  -3.90046  ,   15.716371 ,  -65.88426  ,   85.45233  ,\n",
       "          -72.14462  ],\n",
       "        [  -2.5724378,   13.826544 ,  -56.396496 ,   72.34575  ,\n",
       "          -62.101067 ]], dtype=float32),\n",
       " array([[  -9.765584 ,   43.65421  , -182.47382  ,  236.66553  ,\n",
       "         -200.43744  ],\n",
       "        [  -5.1145625,   21.319834 ,  -91.68277  ,  119.734955 ,\n",
       "         -100.45022  ],\n",
       "        [  -4.065207 ,   16.972792 ,  -70.35678  ,   91.42889  ,\n",
       "          -77.47169  ]], dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 105.13341 ,  -27.344757,  -22.029278,  -97.396484,  -38.40857 ],\n",
       "        [  59.349194,  -15.48696 ,  -13.95987 ,  -54.110474,  -18.848036],\n",
       "        [ 153.63214 ,  -40.30889 ,  -31.816639, -142.6672  ,  -56.442955]],\n",
       "       dtype=float32),\n",
       " array([[ 110.14528 ,  -28.897942,  -22.149921, -102.49854 ,  -41.06072 ],\n",
       "        [  50.561512,  -13.201023,  -11.430093,  -46.29268 ,  -17.780636],\n",
       "        [ 134.92569 ,  -35.47708 ,  -28.584341, -125.3229  ,  -49.612938]],\n",
       "       dtype=float32),\n",
       " array([[ 104.164024,  -26.65059 ,  -23.32068 ,  -95.45499 ,  -35.965065],\n",
       "        [  58.250057,  -15.278679,  -12.216818,  -53.61756 ,  -20.954084],\n",
       "        [ 125.37193 ,  -32.170273,  -27.278997, -115.82131 ,  -44.955193]],\n",
       "       dtype=float32),\n",
       " array([[ 120.78745 ,  -32.132336,  -23.994822, -112.20073 ,  -44.916557],\n",
       "        [  58.981075,  -15.748252,  -12.847244,  -54.7952  ,  -21.025501],\n",
       "        [ 143.18594 ,  -38.1636  ,  -28.778002, -133.8264  ,  -53.82146 ]],\n",
       "       dtype=float32),\n",
       " array([[ 102.98902 ,  -27.148302,  -21.371866,  -95.79752 ,  -38.21783 ],\n",
       "        [  68.97239 ,  -18.681927,  -13.286071,  -64.572685,  -26.494734],\n",
       "        [ 141.05925 ,  -37.522896,  -28.32844 , -131.85158 ,  -53.099854]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
