{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Romain Zimmer\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNet:\n",
    "    \n",
    "    def __init__(self, sess, N, J, embedding_size = 128, lr = 1e-3, training_mode = 'supervised', alpha = 0.03):\n",
    "        \n",
    "        '''\n",
    "        - N: total number of agents\n",
    "        - J: number of levers (and agents randomly selected at each step)\n",
    "        - embedding_size: dimension of the hidden layers \n",
    "        - lr: learning rate \n",
    "        - training_mode: 'supervised' or 'reinforce'\n",
    "        - alpha: paramater used by reinforce training mode to balance reward and baseline loss\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        \n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        if training_mode == 'supervised':\n",
    "            self.build_supervised()\n",
    "            with tf.variable_scope('Supervised_optimizer'):\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "                \n",
    "        elif training_mode == 'reinforce':\n",
    "            self.alpha = 0.03\n",
    "            self.build_reinforce()\n",
    "            with tf.variable_scope('Reinforce_optimizer'):\n",
    "                self.train_op =  tf.train.RMSPropOptimizer(lr).minimize(self.reinforce_loss)\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError(\"Unknown training mode: %s\" % training_mode))            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            self.identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            self.embedded_identities = tf.nn.embedding_lookup(self.identity_embeddings, inputs)\n",
    "        \n",
    "            \n",
    "        return tf.unstack(self.embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None and c is not None:\n",
    "            \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            elif h0 is not None and c is None: \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, h0], axis = 1)\n",
    "                \n",
    "            elif c is not None and h0 is None:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = h\n",
    "                \n",
    "                \n",
    "            b2 = tf.get_variable('b2', shape = (1, self.embedding_size))\n",
    "            \n",
    "            dense1 =tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1) + b1)\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2) + b2)\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1, self.J))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "        \n",
    "            return policy_logit\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        # mean of hidden layers \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_actions(self, log_proba):\n",
    "        \n",
    "        action = tf.multinomial(log_proba, num_samples = 1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        self.h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        # can be used to check values of hidden states \n",
    "        self.hidden_layers = {'h0_seq': h0_seq, 'h1_seq': h1_seq, 'c1_seq':c1_seq, 'h2_seq': self.h2_seq}\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in self.h2_seq]\n",
    "        self.log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.policy_logit_seq]\n",
    "        self.action_seq = [self.sample_actions(log_proba) for log_proba in self.log_proba_seq]\n",
    "        self.one_hot_action_seq = [tf.one_hot(action, depth = self.J) for action in self.action_seq]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_supervised(self):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_mean(supervised_loss_seq)\n",
    "        \n",
    "        \n",
    "    def supervised_train(self, X, y, val_X, val_y, env, batch_size = 32, epochs = 1):\n",
    "    \n",
    "            \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            # shuffle data for each epoch\n",
    "            np.random.shuffle(data_inds)\n",
    "            \n",
    "            supervised_loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm_notebook(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                \n",
    "                # select batch data\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                \n",
    "                # train on batch\n",
    "                _, supervised_loss, one_hot_action_seq = sess.run([self.train_op, self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "               \n",
    "                # keep track of the loss and reward\n",
    "                supervised_loss_sum += supervised_loss * batch_size\n",
    "                reward_sum += env.get_reward(one_hot_action_seq)\n",
    "            \n",
    "            print(\"loss = %f\" % (supervised_loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            # eval loss and reward on validation set\n",
    "            val_supervised_loss, val_one_hot_action_seq = sess.run([self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss))\n",
    "            print('val reward = %f' % (env.get_reward(val_one_hot_action_seq) / val_n))\n",
    "            \n",
    "    def build_baseline(self, h):\n",
    "        \n",
    "        '''state specific baseline for reinforce training mode is given by a simple FC layer\n",
    "        connected to the last hidden layer of the controler\n",
    "        '''\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        with tf.variable_scope('Baseline', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  1))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1,))\n",
    "            \n",
    "            \n",
    "            baseline = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "            \n",
    "            return baseline\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        # only used for scattering \n",
    "        self.indices = tf.placeholder(tf.int32, shape = (None, 2))\n",
    "        self.shape = tf.placeholder(tf.int32, shape =(2,))\n",
    "        \n",
    "        # baseline tensors\n",
    "        self.baselines = tf.concat([self.build_baseline(h2) for h2 in self.h2_seq], axis = 1)\n",
    "        self.scattered_baselines = tf.scatter_nd(self.indices, tf.reshape(self.baselines, [-1]), shape = self.shape)\n",
    "                    \n",
    "        # reward values\n",
    "        self.repeated_reward_values = tf.placeholder(tf.float32, shape = (None,))\n",
    "        self.scattered_reward_values = tf.scatter_nd(self.indices, self.repeated_reward_values, shape = self.shape)\n",
    "        self.scattered_reward_values_cumsum = tf.cumsum(self.scattered_reward_values, axis = 0, reverse = True)\n",
    "        \n",
    "        # baseline values\n",
    "        self.baseline_values =  tf.placeholder(tf.float32, shape = (None, self.J))\n",
    "        self.scattered_baseline_values = tf.scatter_nd(self.indices, tf.reshape(self.baseline_values, [-1]), shape = self.shape)\n",
    "        \n",
    "        # actions that have been taken\n",
    "        self.action_taken = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_action_taken = tf.unstack(self.action_taken, axis = 1)\n",
    "        \n",
    "        # neg log proba of taken actions\n",
    "        self.neg_log_p = tf.transpose(tf.concat([[tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_action_taken[j],\n",
    "                                                    logits=self.policy_logit_seq[j])] for j in range(self.J)], axis = 0))\n",
    "        self.scattered_neg_log_p = tf.scatter_nd(self.indices, tf.reshape(self.neg_log_p, [-1]), shape = self.shape)\n",
    "        \n",
    "        #surrogate loss (- dtheta)\n",
    "        self.reinforce_loss = tf.reduce_sum(tf.multiply(self.scattered_neg_log_p, self.scattered_reward_values_cumsum - self.scattered_baseline_values))\n",
    "        self.reinforce_loss += self.alpha * tf.reduce_sum(tf.square(self.scattered_reward_values_cumsum - self.scattered_baselines))\n",
    "        self.reinforce_loss /= self.J\n",
    "        \n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        action_seq, baselines= self.sess.run([self.action_seq, self.baselines], {self.inputs: [state]})\n",
    "        \n",
    "        return [a[0,0] for a in action_seq], baselines\n",
    "    \n",
    "    def reinforce_train(self, env, n_episodes, T):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        history = {'reward' : [],  'loss': []}    \n",
    "        \n",
    "        for _ in tqdm_notebook(range(n_episodes), \"REINFORCE\"):\n",
    "            \n",
    "            \n",
    "            state_seq, action_seq, reward_seq, baseline_seq = policy_rollout(T, env, self)\n",
    "            episode_len = reward_seq.shape[0]\n",
    "            \n",
    "            history['reward'].append(np.mean(reward_seq))\n",
    "            \n",
    "            repeated_t = np.repeat(np.arange(episode_len), self.J)\n",
    "            \n",
    "            indices = np.vstack([repeated_t, state_seq.ravel()]) .T\n",
    "                \n",
    "            feed_dict = {}\n",
    "            feed_dict[self.inputs] = state_seq\n",
    "            feed_dict[self.indices] = indices\n",
    "            feed_dict[self.shape] = [episode_len, self.N]\n",
    "            feed_dict[self.repeated_reward_values] = np.repeat(reward_seq, self.J)\n",
    "            feed_dict[self.baseline_values] = baseline_seq\n",
    "            feed_dict[self.action_taken] = action_seq\n",
    "            \n",
    "            _, loss = self.sess.run([self.train_op, self.reinforce_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "            \n",
    "        return history\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeverEnv:\n",
    "    \n",
    "    def __init__(self, N, J):\n",
    "        \n",
    "        self.J = J\n",
    "        self.N = N\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return state, terminal_state\n",
    "    \n",
    "    def get_reward(self, one_hot_action_seq):        \n",
    "        \n",
    "        reward = np.sum(np.sum(one_hot_action_seq, axis = 0) > 0) /self.J\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        \n",
    "        next_state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        one_hot_action_seq = np.zeros((self.J, self.J))\n",
    "        one_hot_action_seq[range(self.J), action] = 1\n",
    "        reward = self.get_reward(one_hot_action_seq)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return next_state, reward, terminal_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data(n, N, J):\n",
    "    \n",
    "    '''\n",
    "    Generates data for supervised learning\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    y= np.empty((n,J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.random.choice(N, size = J, replace = False)\n",
    "        sorted_args = np.argsort(X[i])\n",
    "        y[i] = np.argsort(sorted_args)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_rollout(T, env, agent):\n",
    "    \n",
    "    '''\n",
    "    Simulate one episode of length T at most\n",
    "    '''\n",
    "    \n",
    "    state_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    baseline_seq = []\n",
    "    \n",
    "    \n",
    "    state, terminal_state = env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while not terminal_state and t < T:\n",
    "        t +=1\n",
    "        \n",
    "        state_seq.append(state)\n",
    "        action, baseline = agent.take_action(state)\n",
    "        \n",
    "        state, reward, terminal_state = env.step(state, action)\n",
    "        \n",
    "        \n",
    "        action_seq.append(action)\n",
    "        reward_seq.append(reward)\n",
    "        baseline_seq.append(baseline)\n",
    "        \n",
    "    return np.array(state_seq), np.array(action_seq), np.array(reward_seq), np.squeeze(np.array(baseline_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "J = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n = batch_size * 10\n",
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(500, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f1da09ef234ae2b1b19edaf487048d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch: 1', max=10, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss = 1.642637\n",
      "reward = 0.663750\n",
      "\n",
      "val loss = 1.610587\n",
      "val reward = 0.678000\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNet(sess, N, J, lr = 1e-3, embedding_size= 128, training_mode = 'supervised')\n",
    "    env = LeverEnv(N, J)\n",
    "    commNet.supervised_train(X, y, val_X, val_y, env, batch_size = batch_size, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ebbd153b8c49f2b12d8d523fd3b10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='REINFORCE', max=10, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNet(sess, N, J, lr = 1e-3, embedding_size= 128, training_mode = 'reinforce')\n",
    "    env = LeverEnv(N, J)\n",
    "    \n",
    "    history = commNet.reinforce_train(env, n_episodes = 10, T =64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x276dc2896d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x276dc2e66a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Reward')\n",
    "plt.plot(history['reward'], '.')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "plt.title('Loss')\n",
    "plt.plot(history['loss'], '.')\n",
    "plt.xlabel('epoches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
