{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNet:\n",
    "    \n",
    "    def __init__(self, sess, N, J, K, embedding_size = 128, lr = 1e-3):\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.K = K\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "        \n",
    "        for var in tf.global_variables():\n",
    "            print(var)\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            embedded_identities = tf.nn.embedding_lookup(identity_embeddings, inputs)\n",
    "            \n",
    "        return tf.unstack(embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None:\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            else:\n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "            \n",
    "            W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "            \n",
    "            dense1 =  tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1))\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2))\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            policy_logits = tf.einsum(\"ij,jk->ik\", h, W)\n",
    "        \n",
    "            return policy_logits\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_one_hot_actions(self, policy_logit):\n",
    "        \n",
    "        \n",
    "        action = tf.multinomial(policy_logit, num_samples = 1)\n",
    "        \n",
    "        one_hot_action = tf.one_hot(tf.reshape(action, [-1]), depth = self.J)\n",
    "        \n",
    "        return one_hot_action\n",
    "        \n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in h2_seq]\n",
    "\n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_sum(tf.add_n(supervised_loss_seq))\n",
    "        \n",
    "        self.actions = [self.sample_one_hot_actions(policy_logit) for policy_logit in self.policy_logit_seq] \n",
    "        \n",
    "\n",
    "        self.reward = tf.reduce_sum(tf.count_nonzero(tf.add_n(self.actions), axis = 1) / self.J)\n",
    "        \n",
    "        \n",
    "    def train(self, X, y, val_X, val_y, batch_size = 32, epochs = 1):\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            np.random.shuffle(data_inds)\n",
    "            loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                _, loss, reward = sess.run([self.train_op, self.supervised_loss, self.reward], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "                loss_sum += loss\n",
    "                reward_sum += reward\n",
    "                \n",
    "            print(\"loss = %f\" % (loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            val_supervised_loss, val_reward = sess.run([self.supervised_loss, self.reward], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss / val_n))\n",
    "            print('val reward = %f' % (val_reward / val_n))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, N, J):\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.sort(np.random.choice(N, size = J, replace = False))\n",
    "        \n",
    "    y = np.tile([j for j in range(J)], (n,1))\n",
    "        \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "J = 5\n",
    "K = 2\n",
    "batch_size = 32\n",
    "n = batch_size * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(1024, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Encoder/identity_embeddings:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/W:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/beta1_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/beta2_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Encoder/identity_embeddings/Adam:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Encoder/identity_embeddings/Adam_1:0' shape=(100, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_1/W1/Adam:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_1/W1/Adam_1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_1/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_1/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_2/W1/Adam:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_2/W1/Adam_1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_2/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Comm_step_2/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Decoder/W/Adam:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Optimizer/Decoder/W/Adam_1:0' shape=(128, 5) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 91.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 3.753242\n",
      "reward = 0.742094\n",
      "\n",
      "val loss = 2.017161\n",
      "val reward = 0.805273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 99.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.551881\n",
      "reward = 0.838050\n",
      "\n",
      "val loss = 1.128531\n",
      "val reward = 0.872656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 94.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.836161\n",
      "reward = 0.903356\n",
      "\n",
      "val loss = 0.618213\n",
      "val reward = 0.927734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 101.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.443674\n",
      "reward = 0.946125\n",
      "\n",
      "val loss = 0.335846\n",
      "val reward = 0.954492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 96.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.236337\n",
      "reward = 0.969225\n",
      "\n",
      "val loss = 0.200270\n",
      "val reward = 0.973828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:11<00:00, 89.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.133150\n",
      "reward = 0.981781\n",
      "\n",
      "val loss = 0.113380\n",
      "val reward = 0.983789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 96.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.075745\n",
      "reward = 0.989094\n",
      "\n",
      "val loss = 0.078622\n",
      "val reward = 0.989844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 99.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.044250\n",
      "reward = 0.993100\n",
      "\n",
      "val loss = 0.055486\n",
      "val reward = 0.992578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 100.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.026490\n",
      "reward = 0.995512\n",
      "\n",
      "val loss = 0.036524\n",
      "val reward = 0.994531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [00:10<00:00, 98.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.015945\n",
      "reward = 0.997437\n",
      "\n",
      "val loss = 0.047365\n",
      "val reward = 0.995508\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    commNet = CommNet(sess, N, J, K, lr = 1e-4)\n",
    "    commNet.train(X, y, val_X, val_y, batch_size = batch_size, epochs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
