{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.12.0\n",
      "  Downloading https://files.pythonhosted.org/packages/81/07/be624c7e0a63b080a76b7f6faf417eecdc5f6480f6a740a8bcf8991bce0b/tensorflow-1.12.0-cp36-cp36m-macosx_10_11_x86_64.whl (62.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 62.0MB 24kB/s  eta 0:00:01    62% |████████████████████            | 38.9MB 13.8MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 424kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow==1.12.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/27/133f225035b9539f2dcfebcdf9a69ff0152f56e0120160ec5c972ea7deb9/protobuf-3.6.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 880kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorflow==1.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==1.12.0)\n",
      "Requirement already satisfied: h5py in /Users/felixgaschi/anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==1.12.0)\n",
      "Installing collected packages: protobuf, tensorboard, tensorflow\n",
      "  Found existing installation: protobuf 3.5.2.post1\n",
      "    Uninstalling protobuf-3.5.2.post1:\n",
      "      Successfully uninstalled protobuf-3.5.2.post1\n",
      "  Found existing installation: tensorboard 1.7.0\n",
      "    Uninstalling tensorboard-1.7.0:\n",
      "      Successfully uninstalled tensorboard-1.7.0\n",
      "  Found existing installation: tensorflow 1.7.0\n",
      "    Uninstalling tensorflow-1.7.0:\n",
      "      Successfully uninstalled tensorflow-1.7.0\n",
      "Successfully installed protobuf-3.6.1 tensorboard-1.12.2 tensorflow-1.12.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CommNet:\n",
    "    \n",
    "    def __init__(self, sess, N, J, embedding_size = 128, lr = 1e-3, training_mode = 'supervised', alpha = 0.03):\n",
    "        \n",
    "        self.N = N\n",
    "        self.J = J\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build_controler()\n",
    "        \n",
    "        self.training_mode = training_mode\n",
    "        \n",
    "        if training_mode == 'supervised':\n",
    "            self.build_supervised()\n",
    "            with tf.variable_scope('Supervised_optimizer'):\n",
    "                self.train_op = tf.train.AdamOptimizer(lr).minimize(self.supervised_loss)\n",
    "                \n",
    "        elif training_mode == 'reinforce':\n",
    "            self.alpha = 0.03\n",
    "            self.build_reinforce()\n",
    "            with tf.variable_scope('Reinforce_optimizer'):\n",
    "                self.train_op =  tf.train.AdamOptimizer(lr).minimize(self.reinforce_loss)\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError(\"Unknown training mode: %s\" % training_mode))\n",
    "        \n",
    "        print(\"All variables\")\n",
    "        for var in tf.global_variables():\n",
    "            print(var)\n",
    "            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            identity_embeddings = tf.get_variable(\"identity_embeddings\",\n",
    "                                             [self.N, self.embedding_size])\n",
    "            \n",
    "            self.embedded_identities = tf.nn.embedding_lookup(identity_embeddings, inputs)\n",
    "        \n",
    "            \n",
    "        return tf.unstack(self.embedded_identities, axis = 1)\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                W1 = tf.get_variable('W1', shape = (3 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            else:\n",
    "                b1 = tf.get_variable('b1', shape = (1, self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (self.embedding_size,\n",
    "                                                  self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "            \n",
    "            b2 = tf.get_variable('b2', shape = (1, self.embedding_size))\n",
    "            \n",
    "            dense1 =tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1) + b1)\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2) + b2)\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (self.embedding_size,\n",
    "                                                  self.J))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1, self.J))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "        \n",
    "            return policy_logit\n",
    "    \n",
    "    \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        return tf.add_n(h_seq) / (self.J - 1)\n",
    "    \n",
    "    def sample_actions(self, policy_logit):\n",
    "        \n",
    "        action = tf.multinomial(policy_logit, num_samples = 1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        \n",
    "        h0_seq = self.encode(self.inputs)\n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.J)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.J) if j != i]) for i in range(self.J)]\n",
    "        \n",
    "        h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        self.layers = {'h0_seq': h0_seq, 'c0_seq': c0_seq, 'h1_seq': h1_seq, 'c1_seq':c1_seq, 'h2_seq': h2_seq}\n",
    "        \n",
    "        \n",
    "        self.policy_logit_seq = [self.decode(h2) for h2 in h2_seq]\n",
    "        \n",
    "        self.proba_seq = [tf.nn.softmax(policy_logit, axis = 1) for policy_logit in self.policy_logit_seq]\n",
    "        \n",
    "        self.action_seq = [self.sample_actions(policy_logit) for policy_logit in self.policy_logit_seq]\n",
    "        \n",
    "        self.one_hot_action_seq = [tf.one_hot(action, depth = self.J) for action in self.action_seq]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_supervised(self):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        self.targets = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_targets = tf.unstack(self.targets, axis = 1)\n",
    "        \n",
    "        supervised_loss_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_targets[j],\n",
    "                                                                                   logits=self.policy_logit_seq[j])\n",
    "                                    for j in range(self.J)]\n",
    "        \n",
    "        self.supervised_loss = tf.reduce_mean(supervised_loss_seq)\n",
    "        \n",
    "        \n",
    "    def supervised_train(self, X, y, val_X, val_y, env, batch_size = 32, epochs = 1):\n",
    "        \n",
    "        assert self.training_mode == 'supervised', 'Wrong training mode'\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        val_n = val_X.shape[0]\n",
    "        \n",
    "        data_inds = np.array(range(n))\n",
    "        for ep in range(1, epochs + 1):\n",
    "            np.random.shuffle(data_inds)\n",
    "            supervised_loss_sum = 0\n",
    "            reward_sum = 0\n",
    "            for i in tqdm_notebook(range(0, n, batch_size), \"Epoch: %d\" % ep):\n",
    "                inds_batch = data_inds[i:i+batch_size]\n",
    "                X_batch = X[inds_batch]\n",
    "                y_batch = y[inds_batch]\n",
    "                _, supervised_loss, one_hot_action_seq = sess.run([self.train_op, self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: X_batch, self.targets: y_batch})\n",
    "                supervised_loss_sum += supervised_loss * batch_size\n",
    "                reward_sum += env.get_reward(one_hot_action_seq)\n",
    "            \n",
    "            print(\"loss = %f\" % (supervised_loss_sum / n))\n",
    "            print(\"reward = %f\" % (reward_sum / n))\n",
    "            print()\n",
    "            \n",
    "            val_supervised_loss, val_one_hot_action_seq = sess.run([self.supervised_loss, self.one_hot_action_seq], feed_dict={self.inputs: val_X, self.targets: val_y})\n",
    "            print('val loss = %f' % (val_supervised_loss))\n",
    "            print('val reward = %f' % (env.get_reward(val_one_hot_action_seq) / val_n))\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        \n",
    "        self.state_q_val_seq = tf.placeholder(tf.float32, shape = (None, self.J, self.J))\n",
    "        \n",
    "        self.reward_sum_values = tf.placeholder(tf.float32, shape = (None,))\n",
    "        self.advantage_values = tf.placeholder(tf.float32, shape = (None,))\n",
    "  \n",
    "        \n",
    "        self.action_taken = tf.placeholder(tf.int32, shape = (None, self.J))\n",
    "        unstacked_action_taken = tf.unstack(self.action_taken, axis = 1)\n",
    "        one_hot_action_taken_seq = [tf.one_hot(action, depth = self.J, dtype = tf.int32) for action in unstacked_action_taken]\n",
    "        \n",
    "        self.neg_log_p_seq = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_action_taken[j],\n",
    "                                                    logits=self.policy_logit_seq[j]) for j in range(self.J)]\n",
    "        \n",
    "        neg_log_p_sums = tf.reduce_sum(self.neg_log_p_seq, axis = 0)\n",
    "        \n",
    "        \n",
    "        stacked_proba_seq = tf.stack(self.proba_seq, axis = 1)\n",
    "        \n",
    "        baseline = tf.einsum('ijk, ijk-> ij', self.state_q_val_seq, stacked_proba_seq)\n",
    "        baseline = tf.reduce_mean(baseline, axis = 1)\n",
    "        \n",
    "        #surrogate loss (- dtheta)\n",
    "        advantage = self.reward_sum_values - baseline\n",
    "    \n",
    "        self.reinforce_loss =  tf.multiply(neg_log_p_sums, self.advantage_values)\n",
    "        self.reinforce_loss += self.alpha * tf.square(advantage)\n",
    "        self.reinforce_loss = tf.reduce_sum(self.reinforce_loss, axis = 0)\n",
    "        \n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        action_seq, proba_seq = self.sess.run([self.action_seq, self.proba_seq], {self.inputs: [state]})\n",
    "        \n",
    "        return [a[0,0] for a in action_seq], np.array(proba_seq)\n",
    "    \n",
    "    def reinforce_train(self, env, n_episodes, T):\n",
    "        \n",
    "        assert self.training_mode == 'reinforce', 'Wrong training mode'\n",
    "        \n",
    "        \n",
    "        history = {'reward' : [],  'loss': []}\n",
    "        \n",
    "        q_reward_sum = np.zeros((self.N, self.J))\n",
    "        q_state_action_count = np.zeros((self.N, self.J))\n",
    "        q_val = np.zeros((self.N, self.J))\n",
    "    \n",
    "        \n",
    "        for _ in tqdm(range(n_episodes), \"REINFORCE\"):\n",
    "            \n",
    "            \n",
    "            state_seq, action_seq, reward_seq, proba_seq = policy_rollout(T, env, self)\n",
    "            episode_len = reward_seq.shape[0]\n",
    "            \n",
    "            history['reward'].append(np.mean(reward_seq))\n",
    "        \n",
    "            state_q_val_seq = np.array([q_val[state] for state in state_seq])\n",
    "            \n",
    "            baseline = np.einsum('ijk, ijk-> ij', state_q_val_seq, proba_seq)\n",
    "            \n",
    "            baseline = baseline.mean(axis = 1)\n",
    "            \n",
    "            reward_sum_values = np.array([reward_seq[t:].sum() for t in range(episode_len)])\n",
    "            advantage_values = np.array([reward_sum_values[t] - baseline[t] for t in range(episode_len)])          \n",
    "            \n",
    "            \n",
    "            feed_dict = {}\n",
    "            feed_dict[self.inputs] = state_seq\n",
    "            feed_dict[self.state_q_val_seq] = state_q_val_seq\n",
    "            feed_dict[self.reward_sum_values] = reward_sum_values\n",
    "            feed_dict[self.advantage_values] = advantage_values\n",
    "            feed_dict[self.action_taken] = action_seq\n",
    "            \n",
    "            _, loss = self.sess.run([self.train_op, self.reinforce_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "            # udpate_q_val\n",
    "            for i in range(episode_len):\n",
    "                \n",
    "                state = state_seq[i]\n",
    "                action = action_seq[i]\n",
    "                \n",
    "                cummul_reward = reward_seq[i:].sum()\n",
    "                q_state_action_count[state, action] += 1\n",
    "                q_reward_sum[state, action] += cummul_reward\n",
    "            \n",
    "            q_val = q_reward_sum/np.maximum(1,q_state_action_count)\n",
    "            \n",
    "            \n",
    "        return history, q_val\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeverEnv:\n",
    "    \n",
    "    def __init__(self, N, J):\n",
    "        \n",
    "        self.J = J\n",
    "        self.N = N\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return state, terminal_state\n",
    "    \n",
    "    def get_reward(self, one_hot_action_seq):        \n",
    "        \n",
    "        reward = np.sum(np.sum(one_hot_action_seq, axis = 0) > 0) /self.J\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        \n",
    "        next_state = np.random.choice(self.N, size = self.J, replace = False)\n",
    "        \n",
    "        one_hot_action_seq = np.zeros((self.J, self.J))\n",
    "        one_hot_action_seq[range(self.J), action] = 1\n",
    "        reward = self.get_reward(one_hot_action_seq)\n",
    "        \n",
    "        terminal_state = False\n",
    "        \n",
    "        return next_state, reward, terminal_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data generation for supervised learning\n",
    "def generate_data(n, N, J):\n",
    "    \n",
    "    X = np.empty((n, J), dtype = int)\n",
    "    y= np.empty((n,J), dtype = int)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        X[i] = np.random.choice(N, size = J, replace = False)\n",
    "        sorted_args = np.argsort(X[i])\n",
    "        y[i] = np.argsort(sorted_args)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# episode generation for reinforcement learning\n",
    "def policy_rollout(T, env, agent):\n",
    "    \n",
    "    state_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    proba_seq = []\n",
    "    \n",
    "    \n",
    "    state, terminal_state = env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while not terminal_state and t < T:\n",
    "        t +=1\n",
    "        \n",
    "        state_seq.append(state)\n",
    "        action, proba = agent.take_action(state)\n",
    "        \n",
    "        state, reward, terminal_state = env.step(state, action)\n",
    "        \n",
    "        \n",
    "        action_seq.append(action)\n",
    "        reward_seq.append(reward)\n",
    "        proba_seq.append(proba)\n",
    "        \n",
    "    return np.array(state_seq), np.array(action_seq), np.array(reward_seq), np.squeeze(np.array(proba_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 500\n",
    "J = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size * 1000\n",
    "X, y = generate_data(n, N, J)\n",
    "val_X, val_y = generate_data(500, N, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "REINFORCE:   0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variables\n",
      "<tf.Variable 'Encoder/identity_embeddings:0' shape=(500, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b2:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W2:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b2:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/W:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder/b:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/beta1_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/beta2_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/Adam:0' shape=(500, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/identity_embeddings/Adam_1:0' shape=(500, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/Adam:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/Adam_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/Adam:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/Adam_1:0' shape=(256, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/Adam:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/Adam_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/Adam:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/Adam_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/Adam:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/Adam_1:0' shape=(384, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/Adam:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/Adam_1:0' shape=(128, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/Adam:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/Adam_1:0' shape=(1, 128) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/Adam:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/W/Adam_1:0' shape=(128, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/b/Adam:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder/b/Adam_1:0' shape=(1, 5) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE:   7%|▋         | 3470/50000 [05:09<1:09:14, 11.20it/s]"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNet(sess, N, J, lr = 1e-3, embedding_size= 128, training_mode = 'reinforce', alpha = 0.03)\n",
    "    env = LeverEnv(N, J)\n",
    "    #commNet.supervised_train(X, y, val_X, val_y, env, batch_size = batch_size, epochs = 30)\n",
    "    \n",
    "    history, q_val = commNet.reinforce_train(env, n_episodes = 50000, T = 64)\n",
    "    state_seq, action_seq, reward_seq, proba_seq = policy_rollout(T = 64, env = env, agent = commNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history['reward'], '*')\n",
    "plt.show()\n",
    "plt.plot(history['loss'], '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!notifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
