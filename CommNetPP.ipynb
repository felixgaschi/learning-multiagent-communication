{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommNetPP:\n",
    "    \n",
    "    def __init__(self, sess, N, grid_size, embedding_size = 50, lr = 1e-3, alpha = 0.03, lstm_controler = False):\n",
    "        \n",
    "        self.N = N\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_actions = 6 # Stay, North, South, East, West, Communicate\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        if lstm_controler:\n",
    "            self.build_lstm_controler()\n",
    "        else:\n",
    "            self.build_controler()\n",
    "        \n",
    "        \n",
    "        self.alpha = 0.03\n",
    "        self.build_reinforce()\n",
    "        with tf.variable_scope('Reinforce_optimizer'):\n",
    "            self.train_op =  tf.train.RMSPropOptimizer(lr).minimize(self.reinforce_loss)\n",
    "            \n",
    "        \n",
    "        print(\"All variables\")\n",
    "        for var in tf.global_variables():\n",
    "            print(var)\n",
    "            \n",
    "        \n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def encode(self, inputs):\n",
    "        \n",
    "        agent_positions = inputs[0]\n",
    "        prey_positions = inputs[1]\n",
    "        \n",
    "        print(agent_positions)\n",
    "        print(prey_positions)\n",
    "        \n",
    "        with tf.variable_scope('Encoder'):\n",
    "        \n",
    "            self.agent_position_embedding = tf.get_variable(\"agent_position_embedding\",\n",
    "                                             [self.grid_size**2 + 1, self.embedding_size]) # 0 not used\n",
    "            \n",
    "            self.embedded_agent_position = tf.nn.embedding_lookup(self.agent_position_embedding, agent_positions)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            self.prey_position_embedding = tf.get_variable(\"prey_position_embedding\",\n",
    "                                             [self.grid_size**2 + 1, self.embedding_size]) # 0 --> cannot see the prey\n",
    "            \n",
    "            self.embedded_prey_position = tf.nn.embedding_lookup(self.prey_position_embedding, prey_positions)\n",
    "        \n",
    "        \n",
    "        print(self.embedded_prey_position)\n",
    "        h0 = tf.concat([self.embedded_agent_position, self.embedded_prey_position], axis = 2)\n",
    "        h0_seq = tf.unstack(h0, axis = 1)\n",
    "        \n",
    "        return h0_seq\n",
    "    \n",
    "    def build_f(self, name, h, c, h0 = None):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            if h0 is not None and c is not None:\n",
    "            \n",
    "                b1 = tf.get_variable('b1', shape = (1, 2 * self.embedding_size))\n",
    "                W1 = tf.get_variable('W1', shape = (3 * 2 * self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (2 * self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            elif h0 is not None and c is None: \n",
    "                b1 = tf.get_variable('b1', shape = (1, 2* self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * 2 * self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (2 * self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, h0], axis = 1)\n",
    "                \n",
    "            elif c is not None and h0 is None:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, 2 * self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * 2 *self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (2 * self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                concat = tf.concat([h, c], axis = 1)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                b1 = tf.get_variable('b1', shape = (1, 2 *self.embedding_size))\n",
    "                \n",
    "                W1 = tf.get_variable('W1', shape = (2 * self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                W2 = tf.get_variable('W2', shape = (2 * self.embedding_size,\n",
    "                                                  2 * self.embedding_size))\n",
    "                \n",
    "                concat = h\n",
    "                \n",
    "                \n",
    "            b2 = tf.get_variable('b2', shape = (1, 2 * self.embedding_size))\n",
    "            \n",
    "            dense1 =tf.nn.relu(tf.einsum(\"ij,jk->ik\", concat, W1) + b1)\n",
    "            dense2 = tf.nn.relu(tf.einsum(\"ij,jk->ik\", dense1, W2) + b2)\n",
    "            \n",
    "            return dense2\n",
    "        \n",
    "    def decode_movement(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder_movement', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (2 * self.embedding_size,\n",
    "                                                  self.n_actions - 1))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1, self.n_actions - 1))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "        \n",
    "            return policy_logit\n",
    "        \n",
    "    def decode_communication(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Decoder_communication', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (2 * self.embedding_size,\n",
    "                                                  2))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1, 2))\n",
    "            \n",
    "            policy_logit = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "        \n",
    "            return policy_logit\n",
    "        \n",
    "    def communicate(self, h_seq):\n",
    "        \n",
    "        return tf.add_n(h_seq) / (self.N - 1)\n",
    "    \n",
    "    def sample_actions(self, log_proba):\n",
    "        \n",
    "        action = tf.multinomial(log_proba, num_samples = 1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "        \n",
    "    def build_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (1, 2, self.N))\n",
    "        \n",
    "        h0_seq = self.encode(tf.unstack(self.inputs, axis = 1))\n",
    "        \n",
    "        print(h0_seq)\n",
    "        \n",
    "        c0_seq = [self.communicate([h0_seq[j] for j in range(self.N) if j != i]) for i in range(self.N)]\n",
    "        \n",
    "        h1_seq = [self.build_f(\"Comm_step_1\", h0_seq[j], c0_seq[j], None) for j in range(self.N)]\n",
    "        c1_seq = [self.communicate([h1_seq[j] for j in range(self.N) if j != i]) for i in range(self.N)]\n",
    "        \n",
    "        self.h2_seq = [self.build_f(\"Comm_step_2\", h1_seq[j], c1_seq[j], h0_seq[j]) for j in range(self.N)]\n",
    "        \n",
    "        self.layers = {'h0_seq': h0_seq, 'h1_seq': h1_seq, 'c1_seq':c1_seq, 'h2_seq': self.h2_seq}\n",
    "        \n",
    "        \n",
    "        self.movement_policy_logit_seq = [self.decode_movement(h2) for h2 in self.h2_seq]\n",
    "        self.communication_policy_logit_seq = [self.decode_communication(h2) for h2 in self.h2_seq]\n",
    "        \n",
    "        self.movement_log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.movement_policy_logit_seq]\n",
    "        self.communication_log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.communication_policy_logit_seq]\n",
    "        \n",
    "        self.movement_action_seq = [self.sample_actions(log_proba) for log_proba in self.movement_log_proba_seq]\n",
    "        self.communication_action_seq = [self.sample_actions(log_proba) for log_proba in self.communication_log_proba_seq]\n",
    "        \n",
    "        print(self.movement_action_seq)\n",
    "        print(self.movement_action_seq)\n",
    "        \n",
    "    def build_lstm(self, name, cell_state):\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(2 * 2 * self.embedding_size)\n",
    "            concat = tf.concat([h, c, h0], axis = 1)\n",
    "            \n",
    "            tf.nn.dynamic_rnn(inputs = concat, cell = lstm_cell, initial_state = cell_state)\n",
    "            \n",
    "            #TODO --> gérer le state du RNN\n",
    "    \n",
    "    def build_lstm_controler(self):\n",
    "        \n",
    "        self.inputs = tf.placeholder(tf.int32, shape = (1, 2, self.N))\n",
    "        \n",
    "        h_seq = self.encode(tf.unstack(self.inputs, axis = 1))\n",
    "        \n",
    "        print(h_seq)\n",
    "        \n",
    "        c_seq = [self.communicate([h_seq[j] for j in range(self.N) if j != i]) for i in range(self.N)]\n",
    "        \n",
    "        h_seq = [self.build_lstm(\"Comm_step_lstm\", h0_seq[j], c_seq[j]) for j in range(self.N)]\n",
    "                \n",
    "        \n",
    "        self.movement_policy_logit_seq = [self.decode_movement(h) for h in self.h_seq]\n",
    "        self.communication_policy_logit_seq = [self.decode_communication(h) for h in self.h_seq]\n",
    "        \n",
    "        self.movement_log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.movement_policy_logit_seq]\n",
    "        self.communication_log_proba_seq = [tf.nn.log_softmax(policy_logit, axis = 1) for policy_logit in self.communication_policy_logit_seq]\n",
    "        \n",
    "        self.movement_action_seq = [self.sample_actions(log_proba) for log_proba in self.movement_log_proba_seq]\n",
    "        self.communication_action_seq = [self.sample_actions(log_proba) for log_proba in self.communication_log_proba_seq]\n",
    "        \n",
    "        print(self.movement_action_seq)\n",
    "        print(self.movement_action_seq)        \n",
    "        \n",
    "            \n",
    "    def build_baseline(self, h):\n",
    "        \n",
    "        with tf.variable_scope('Baseline', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            W = tf.get_variable('W', shape = (2 * self.embedding_size,\n",
    "                                                  1))\n",
    "            \n",
    "            b = tf.get_variable('b', shape = (1,))\n",
    "            \n",
    "            \n",
    "            baseline = tf.einsum(\"ij,jk->ik\", h, W) + b\n",
    "            \n",
    "            return baseline\n",
    "            \n",
    "\n",
    "    def build_reinforce(self):\n",
    "        \n",
    "        \n",
    "        self.baselines = tf.concat([self.build_baseline(h2) for h2 in self.h2_seq], axis = 1)\n",
    "                    \n",
    "        self.reward_values = tf.placeholder(tf.float32, shape = (None, self.N))\n",
    "        self.reward_values_cumsum = tf.cumsum(self.reward_values, axis = 0, reverse = True)\n",
    "        \n",
    "        self.baseline_values =  tf.placeholder(tf.float32, shape = (None, self.N))\n",
    "               \n",
    "        \n",
    "        self.movement_action_taken = tf.placeholder(tf.int32, shape = (None, self.N))\n",
    "        unstacked_movement_action_taken = tf.unstack(self.movement_action_taken, axis = 1)\n",
    "        \n",
    "        self.communication_action_taken = tf.placeholder(tf.int32, shape = (None, self.N))\n",
    "        unstacked_communication_action_taken = tf.unstack(self.communication_action_taken, axis = 1)\n",
    "        \n",
    "        self.movement_neg_log_p = tf.transpose(tf.concat([[tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_movement_action_taken[j],\n",
    "                                                    logits=self.movement_policy_logit_seq[j])] for j in range(self.N)], axis = 0))\n",
    "        \n",
    "        self.communication_action_taken = tf.placeholder(tf.int32, shape = (None, self.N))\n",
    "        unstacked_communication_action_taken = tf.unstack(self.communication_action_taken, axis = 1)\n",
    "        \n",
    "        self.communication_neg_log_p = tf.transpose(tf.concat([[tf.nn.sparse_softmax_cross_entropy_with_logits(labels=unstacked_communication_action_taken[j],\n",
    "                                                    logits=self.communication_policy_logit_seq[j])] for j in range(self.N)], axis = 0))\n",
    "        \n",
    "        self.neg_log_p = self.movement_neg_log_p + self.communication_neg_log_p\n",
    "        #surrogate loss (- dtheta)\n",
    "        \n",
    "        self.reinforce_loss = tf.reduce_sum(tf.multiply(self.neg_log_p, self.reward_values_cumsum - self.baseline_values))\n",
    "        self.reinforce_loss += self.alpha * tf.reduce_sum(tf.square(self.reward_values_cumsum - self.baselines))\n",
    "        self.reinforce_loss /= self.N\n",
    "        \n",
    "        print(self.reinforce_loss)\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        \n",
    "        movement_action_seq, communication_action_seq, baselines= self.sess.run([self.movement_action_seq, self.communication_action_seq, self.baselines], {self.inputs: [state]})\n",
    "        \n",
    "        return [a[0,0] for a in movement_action_seq], [a[0,0] for a in communication_action_seq],  baselines\n",
    "    \n",
    "    def reinforce_train(self, env, n_episodes, T):\n",
    "        \n",
    "        \n",
    "        history = {'reward' : [],  'loss': []}\n",
    "        \n",
    "        for _ in tqdm_notebook(range(n_episodes), \"REINFORCE\"):\n",
    "            \n",
    "            \n",
    "            state_seq, movement_action_seq, communication_action_seq , reward_seq, baseline_seq = policy_rollout(T, env, self)\n",
    "            episode_len = reward_seq.shape[0]\n",
    "            \n",
    "            history['reward'].append(np.mean(reward_seq))\n",
    "            \n",
    "            repeated_t = np.repeat(np.arange(episode_len), self.N)\n",
    "            \n",
    "            indices = np.vstack([repeated_t, state_seq.ravel()]) .T\n",
    "                \n",
    "            feed_dict = {}\n",
    "            feed_dict[self.inputs] = state_seq\n",
    "            feed_dict[self.reward_values] = reward_seq\n",
    "            feed_dict[self.baseline_values] = baseline_seq\n",
    "            feed_dict[self.movement_action_taken] = movement_action_seq\n",
    "            feed_dict[self.communication_action_taken] = communication_action_seq\n",
    "            \n",
    "            _, loss = self.sess.run([self.train_op, self.reinforce_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            \n",
    "        return history\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode generation for reinforcement learning\n",
    "def policy_rollout(T, env, agent):\n",
    "    \n",
    "    state_seq = []\n",
    "    movement_action_seq = []\n",
    "    communication_action_seq = []\n",
    "    reward_seq = []\n",
    "    baseline_seq = []\n",
    "    \n",
    "    \n",
    "    state, terminal_state = env.reset()\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    while not terminal_state and t < T:\n",
    "        t +=1\n",
    "        \n",
    "        state_seq.append(state)\n",
    "        movement_action, communication_action, baseline = agent.take_action(state)\n",
    "        \n",
    "        state, reward, terminal_state = env.step(movement_action, communication_action)\n",
    "        \n",
    "        movement_action_seq.append(movement_action)\n",
    "        communication_action_seq.append(communication_action)\n",
    "        reward_seq.append(reward)\n",
    "        baseline_seq.append(baseline)\n",
    "        \n",
    "    return np.array(state_seq), np.array(movement_action_seq), np.array(communication_action_seq),np.array(reward_seq), np.squeeze(np.array(baseline_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "grid_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"unstack:0\", shape=(1, 4), dtype=int32)\n",
      "Tensor(\"unstack:1\", shape=(1, 4), dtype=int32)\n",
      "Tensor(\"Encoder/embedding_lookup_1/Identity:0\", shape=(1, 4, 50), dtype=float32)\n",
      "[<tf.Tensor 'unstack_1:0' shape=(1, 100) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(1, 100) dtype=float32>, <tf.Tensor 'unstack_1:2' shape=(1, 100) dtype=float32>, <tf.Tensor 'unstack_1:3' shape=(1, 100) dtype=float32>]\n",
      "[<tf.Tensor 'multinomial/Multinomial:0' shape=(1, 1) dtype=int64>, <tf.Tensor 'multinomial_1/Multinomial:0' shape=(1, 1) dtype=int64>, <tf.Tensor 'multinomial_2/Multinomial:0' shape=(1, 1) dtype=int64>, <tf.Tensor 'multinomial_3/Multinomial:0' shape=(1, 1) dtype=int64>]\n",
      "[<tf.Tensor 'multinomial/Multinomial:0' shape=(1, 1) dtype=int64>, <tf.Tensor 'multinomial_1/Multinomial:0' shape=(1, 1) dtype=int64>, <tf.Tensor 'multinomial_2/Multinomial:0' shape=(1, 1) dtype=int64>, <tf.Tensor 'multinomial_3/Multinomial:0' shape=(1, 1) dtype=int64>]\n",
      "Tensor(\"truediv_8:0\", shape=(), dtype=float32)\n",
      "All variables\n",
      "<tf.Variable 'Encoder/agent_position_embedding:0' shape=(401, 50) dtype=float32_ref>\n",
      "<tf.Variable 'Encoder/prey_position_embedding:0' shape=(401, 50) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b1:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W1:0' shape=(200, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/W2:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_1/b2:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b1:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W1:0' shape=(300, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/W2:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Comm_step_2/b2:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder_movement/W:0' shape=(100, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder_movement/b:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder_communication/W:0' shape=(100, 2) dtype=float32_ref>\n",
      "<tf.Variable 'Decoder_communication/b:0' shape=(1, 2) dtype=float32_ref>\n",
      "<tf.Variable 'Baseline/W:0' shape=(100, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Baseline/b:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/agent_position_embedding/RMSProp:0' shape=(401, 50) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/agent_position_embedding/RMSProp_1:0' shape=(401, 50) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/prey_position_embedding/RMSProp:0' shape=(401, 50) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Encoder/prey_position_embedding/RMSProp_1:0' shape=(401, 50) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/RMSProp:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b1/RMSProp_1:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/RMSProp:0' shape=(200, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W1/RMSProp_1:0' shape=(200, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/RMSProp:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/W2/RMSProp_1:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/RMSProp:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_1/b2/RMSProp_1:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/RMSProp:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b1/RMSProp_1:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/RMSProp:0' shape=(300, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W1/RMSProp_1:0' shape=(300, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/RMSProp:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/W2/RMSProp_1:0' shape=(100, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/RMSProp:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Comm_step_2/b2/RMSProp_1:0' shape=(1, 100) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_movement/W/RMSProp:0' shape=(100, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_movement/W/RMSProp_1:0' shape=(100, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_movement/b/RMSProp:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_movement/b/RMSProp_1:0' shape=(1, 5) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_communication/W/RMSProp:0' shape=(100, 2) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_communication/W/RMSProp_1:0' shape=(100, 2) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_communication/b/RMSProp:0' shape=(1, 2) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Decoder_communication/b/RMSProp_1:0' shape=(1, 2) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/W/RMSProp:0' shape=(100, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/W/RMSProp_1:0' shape=(100, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/b/RMSProp:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'Reinforce_optimizer/Baseline/b/RMSProp_1:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    commNet = CommNetPP(sess, N,  grid_size, lstm_module = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
